"""
å®æ—¶æ£€æµ‹æœåŠ¡æ¨¡å—
Real-time detection service using YOLOv11 for classroom behavior detection.
æ”¯æŒå¤šçº¿ç¨‹å¤„ç†ä»¥æé«˜å¸§ç‡
æ•´åˆäº†æ•°æ®å­˜å‚¨åŠŸèƒ½ï¼Œç¬¦åˆServiceå±‚èŒè´£
"""
import logging
import base64
import cv2
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import os
import sys
from PIL import Image, ImageDraw, ImageFont
from concurrent.futures import ThreadPoolExecutor
import threading
import time

# å¯¼å…¥æ•°æ®è®¿é—®å±‚ç»„ä»¶
from ..model.ManagerModel import DatabaseManager
from ..model.ConfigModel import DatabaseConfig
from backend.model.Detection_accessModel import DetectionDataAccess

# å¯¼å…¥æœåŠ¡æ¥å£
from .InterfaceService import IDetectionService

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

logger = logging.getLogger(__name__)

# è¡Œä¸ºç±»åˆ«é…ç½® - ä¸è®­ç»ƒæ¨¡å‹ä¸€è‡´
BEHAVIOR_CLASSES = {
    0: {'name': 'handrise', 'cn_name': 'ä¸¾æ‰‹', 'type': 'normal', 'color': (0, 255, 0)},
    # 1: {'name': 'read', 'cn_name': 'é˜…è¯»', 'type': 'normal', 'color': (0, 200, 0)},  # å·²ç¦ç”¨
    2: {'name': 'write', 'cn_name': 'ä¹¦å†™', 'type': 'normal', 'color': (0, 180, 0)},
    3: {'name': 'sleep', 'cn_name': 'ç¡è§‰', 'type': 'warning', 'color': (255, 0, 0)},
    4: {'name': 'stand', 'cn_name': 'ç«™ç«‹', 'type': 'warning', 'color': (128, 128, 128)},
    5: {'name': 'using_electronic_devices', 'cn_name': 'ä½¿ç”¨ç”µå­è®¾å¤‡', 'type': 'warning', 'color': (255, 0, 255)},
    6: {'name': 'talk', 'cn_name': 'äº¤è°ˆ', 'type': 'warning', 'color': (255, 165, 0)},
    7: {'name': 'head_down', 'cn_name': 'ä½å¤´', 'type': 'warning', 'color': (255, 128, 0)},  # æ–°å¢ä½å¤´è¡Œä¸º
}

# é¢„è­¦çº§åˆ«
ALERT_LEVELS = {
    0: {'name': 'normal', 'cn_name': 'æ­£å¸¸', 'classes': [0, 2]},  # ç§»é™¤é˜…è¯»(1)
    1: {'name': 'mild', 'cn_name': 'è½»åº¦é¢„è­¦', 'classes': [4, 7]},
    2: {'name': 'moderate', 'cn_name': 'ä¸­åº¦é¢„è­¦', 'classes': [6]},
    3: {'name': 'severe', 'cn_name': 'ä¸¥é‡é¢„è­¦', 'classes': [3, 5]},
}


@dataclass
class Detection:
    """æ£€æµ‹ç»“æœ"""
    class_id: int
    class_name: str
    class_name_cn: str
    confidence: float
    bbox: List[float]  # [x1, y1, x2, y2]
    behavior_type: str
    alert_level: int
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœæ±‡æ€»"""
    detections: List[Detection]
    total_count: int
    warning_count: int
    normal_count: int
    behavior_summary: Dict[str, int]
    alert_summary: Dict[str, int]
    timestamp: str
    behavior_duration: Dict[str, float] = None  # å„è¡Œä¸ºç´¯è®¡æ—¶é—´ï¼ˆç§’ï¼‰
    
    def to_dict(self) -> Dict:
        result = {
            'detections': [d.to_dict() for d in self.detections],
            'total_count': self.total_count,
            'warning_count': self.warning_count,
            'normal_count': self.normal_count,
            'behavior_summary': self.behavior_summary,
            'alert_summary': self.alert_summary,
            'timestamp': self.timestamp
        }
        if self.behavior_duration:
            result['behavior_duration'] = self.behavior_duration
        return result


class FPSCounter:
    """FPSè®¡æ•°å™¨"""
    def __init__(self, avg_frames: int = 30):
        self.avg_frames = avg_frames
        self.timestamps = []
        self._lock = threading.Lock()
    
    def tick(self):
        """è®°å½•ä¸€å¸§"""
        with self._lock:
            now = time.time()
            self.timestamps.append(now)
            # åªä¿ç•™æœ€è¿‘çš„å¸§
            if len(self.timestamps) > self.avg_frames:
                self.timestamps = self.timestamps[-self.avg_frames:]
    
    def get_fps(self) -> float:
        """è·å–å½“å‰FPS"""
        with self._lock:
            if len(self.timestamps) < 2:
                return 0.0
            duration = self.timestamps[-1] - self.timestamps[0]
            if duration <= 0:
                return 0.0
            return (len(self.timestamps) - 1) / duration


class BehaviorTimeTracker:
    """
    è¡Œä¸ºæ—¶é—´è·Ÿè¸ªå™¨
    ç”¨äºç»Ÿè®¡å„ç§è¡Œä¸ºçš„ç´¯è®¡æŒç»­æ—¶é—´
    """
    
    def __init__(self):
        self.start_time = datetime.now()
        self.last_update_time = datetime.now()
        self.behavior_duration = {info['cn_name']: 0.0 for info in BEHAVIOR_CLASSES.values()}
        self.frame_count = 0
        self.detection_interval = 0.5  # é»˜è®¤æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰
    
    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.start_time = datetime.now()
        self.last_update_time = datetime.now()
        self.behavior_duration = {info['cn_name']: 0.0 for info in BEHAVIOR_CLASSES.values()}
        self.frame_count = 0
    
    def update(self, detections: List[Detection], interval_seconds: float = None):
        """
        æ›´æ–°è¡Œä¸ºæ—¶é—´ç»Ÿè®¡
        
        Args:
            detections: å½“å‰å¸§æ£€æµ‹åˆ°çš„è¡Œä¸ºåˆ—è¡¨
            interval_seconds: æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼
        """
        current_time = datetime.now()
        
        # è®¡ç®—æ—¶é—´é—´éš”
        if interval_seconds is not None:
            delta = interval_seconds
        else:
            delta = (current_time - self.last_update_time).total_seconds()
            # é™åˆ¶æœ€å¤§é—´éš”ä¸º2ç§’ï¼Œé¿å…å¼‚å¸¸å€¼
            delta = min(delta, 2.0)
        
        self.last_update_time = current_time
        self.frame_count += 1
        
        # ç»Ÿè®¡å½“å‰å¸§æ£€æµ‹åˆ°çš„è¡Œä¸º
        current_behaviors = set()
        for det in detections:
            current_behaviors.add(det.class_name_cn)
        
        # ä¸ºæ£€æµ‹åˆ°çš„è¡Œä¸ºç´¯åŠ æ—¶é—´
        for behavior_name in current_behaviors:
            if behavior_name in self.behavior_duration:
                self.behavior_duration[behavior_name] += delta
    
    def get_duration(self) -> Dict[str, float]:
        """è·å–å„è¡Œä¸ºç´¯è®¡æ—¶é—´ï¼ˆç§’ï¼‰"""
        return {k: round(v, 1) for k, v in self.behavior_duration.items()}
    
    def get_duration_formatted(self) -> Dict[str, str]:
        """è·å–æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²ï¼ˆåˆ†:ç§’ï¼‰"""
        result = {}
        for name, seconds in self.behavior_duration.items():
            minutes = int(seconds // 60)
            secs = int(seconds % 60)
            result[name] = f"{minutes}:{secs:02d}"
        return result
    
    def get_total_time(self) -> float:
        """è·å–æ€»æ£€æµ‹æ—¶é—´ï¼ˆç§’ï¼‰"""
        return (datetime.now() - self.start_time).total_seconds()
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´ç»Ÿè®¡ä¿¡æ¯"""
        total_time = self.get_total_time()
        return {
            'total_time': round(total_time, 1),
            'total_time_formatted': f"{int(total_time // 60)}:{int(total_time % 60):02d}",
            'frame_count': self.frame_count,
            'behavior_duration': self.get_duration(),
            'behavior_duration_formatted': self.get_duration_formatted()
        }


class DetectionService(IDetectionService):
    """
    å®æ—¶æ£€æµ‹æœåŠ¡
    Provides real-time behavior detection using YOLO models.
    æ”¯æŒåŒæ¨¡å‹æ£€æµ‹ï¼šè¡Œä¸ºæ¨¡å‹ + ç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼ˆæ£€æµ‹ç”µå­è®¾å¤‡ï¼‰
    æ”¯æŒäººè„¸æ£€æµ‹åˆ¤æ–­ä½å¤´è¡Œä¸º
    æ”¯æŒå¤šçº¿ç¨‹å¤„ç†ä»¥æé«˜å¸§ç‡
    """
    
    # COCOæ•°æ®é›†ä¸­çš„ç”µå­è®¾å¤‡ç±»åˆ«ï¼ˆä»…æ£€æµ‹éå­¦ä¹ ç”¨é€”çš„è®¾å¤‡ï¼‰
    # å¯¹äºè®¡ç®—æœºä¸“ä¸šå­¦ç”Ÿï¼Œç”µè„‘å’Œç¬”è®°æœ¬æ˜¯æ­£å¸¸å­¦ä¹ å·¥å…·ï¼Œä¸ä½œä¸ºé¢„è­¦
    ELECTRONIC_DEVICE_CLASSES = {
        67: 'cell phone',      # æ‰‹æœº - éœ€è¦æ£€æµ‹
        # 63: 'laptop',        # ç¬”è®°æœ¬ç”µè„‘ - è®¡ç®—æœºä¸“ä¸šæ­£å¸¸ä½¿ç”¨ï¼Œå·²ç¦ç”¨
        # 62: 'tv',            # ç”µè§†/æ˜¾ç¤ºå™¨ - è®¡ç®—æœºä¸“ä¸šæ­£å¸¸ä½¿ç”¨ï¼Œå·²ç¦ç”¨
        # 66: 'keyboard',      # é”®ç›˜ - è®¡ç®—æœºä¸“ä¸šæ­£å¸¸ä½¿ç”¨ï¼Œå·²ç¦ç”¨
        # 64: 'mouse',         # é¼ æ ‡ - è®¡ç®—æœºä¸“ä¸šæ­£å¸¸ä½¿ç”¨ï¼Œå·²ç¦ç”¨
        # 74: 'remote',        # é¥æ§å™¨ - å·²ç¦ç”¨
    }
    
    # COCOæ•°æ®é›†ä¸­çš„äººç±»åˆ«
    PERSON_CLASS_ID = 0
    
    def __init__(self, model_path: str = None, db: DatabaseManager = None, config: DatabaseConfig = None):
        """
        åˆå§‹åŒ–æ£€æµ‹æœåŠ¡
        
        Args:
            model_path: YOLOæ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®ä¸­è®­ç»ƒå¥½çš„æ¨¡å‹
            db: æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
            config: æ•°æ®åº“é…ç½®
        """
        # YOLOæ£€æµ‹ç›¸å…³åˆå§‹åŒ–
        self.model = None
        self.device_model = None  # ç”µå­è®¾å¤‡æ£€æµ‹æ¨¡å‹
        self.face_cascade = None  # äººè„¸æ£€æµ‹å™¨
        self.model_path = model_path
        self.confidence_threshold = 0.45  # æé«˜é»˜è®¤ç½®ä¿¡åº¦é˜ˆå€¼ä»¥å‡å°‘è¯¯æ£€æµ‹
        self.iou_threshold = 0.5  # æé«˜IOUé˜ˆå€¼ä»¥å‡å°‘é‡å æ¡†
        self.model_loaded = False
        self.device_model_loaded = False
        self.device = self._get_device()  # æ£€æµ‹è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        self.time_tracker = BehaviorTimeTracker()  # è¡Œä¸ºæ—¶é—´è·Ÿè¸ªå™¨
        
        # æ•°æ®å­˜å‚¨ç›¸å…³åˆå§‹åŒ–
        if db is None:
            self.data_access = DetectionDataAccess(config=config)
        else:
            self.data_access = DetectionDataAccess(db=db)
        
        # å½“å‰ä¼šè¯çŠ¶æ€
        self._current_session_id: Optional[int] = None
        self._frame_count: int = 0
        self._record_buffer: List[Dict] = []
        self._entry_buffer: List[Dict] = []
        self._buffer_size: int = 100  # æ‰¹é‡æ’å…¥é˜ˆå€¼
        
        # GPU ä¼˜åŒ–å‚æ•°
        self.use_half = self.device != 'cpu'  # GPU æ—¶ä½¿ç”¨ FP16 åŠç²¾åº¦
        self.imgsz = 1280  # æ¨ç†å›¾åƒå°ºå¯¸ï¼ˆå¢å¤§ä»¥æé«˜ GPU åˆ©ç”¨ç‡ï¼‰
        
        # å¤šçº¿ç¨‹ç›¸å…³
        self._executor = ThreadPoolExecutor(max_workers=3)  # çº¿ç¨‹æ± 
        self._detection_lock = threading.Lock()  # æ£€æµ‹é”
        self._last_result = None  # ç¼“å­˜æœ€åä¸€æ¬¡æ£€æµ‹ç»“æœ
        self._last_annotated_image = None  # ç¼“å­˜æœ€åä¸€æ¬¡æ ‡æ³¨å›¾åƒ
        self._frame_skip = 2  # è·³å¸§æ•°ï¼ˆæ¯Nå¸§æ£€æµ‹ä¸€æ¬¡ï¼‰
        self._frame_count_detection = 0  # æ£€æµ‹å¸§è®¡æ•°å™¨ï¼ˆåŒºåˆ«äºæ•°æ®åº“å¸§è®¡æ•°ï¼‰
        self._fps_counter = FPSCounter()  # FPSè®¡æ•°å™¨
        
        self._load_model()
        self._load_device_model()
        self._load_face_detector()
    
    def _get_device(self) -> str:
        """è·å–æœ€ä½³è®¡ç®—è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰"""
        try:
            import torch
            if torch.cuda.is_available():
                device_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"Using GPU: {device_name} ({gpu_memory:.1f}GB)")
                return 'cuda:0'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
            else:
                logger.info("CUDA not available, using CPU")
                return 'cpu'
        except Exception as e:
            logger.warning(f"Failed to detect GPU: {e}, using CPU")
            return 'cpu'
    
    def _load_model(self):
        """åŠ è½½YOLOæ¨¡å‹"""
        try:
            from ultralytics import YOLO
            
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            
            # ä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹è·¯å¾„
            if self.model_path and os.path.exists(self.model_path):
                self.model = YOLO(self.model_path)
                self.model.to(self.device)  # ç§»åŠ¨åˆ°GPU
                if self.use_half and self.device != 'cpu':
                    self.model.model.half()  # å¯ç”¨ FP16 åŠç²¾åº¦
                    logger.info("Model using FP16 half precision")
                logger.info(f"Loaded model from {self.model_path} on device {self.device}")
                self.model_loaded = True
                return
            
            # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
            trained_model_path = os.path.join(project_root, 'runs/detect/classroom_behavior_4050/weights/best.pt')
            if os.path.exists(trained_model_path):
                self.model = YOLO(trained_model_path)
                self.model.to(self.device)  # ç§»åŠ¨åˆ°GPU
                if self.use_half and self.device != 'cpu':
                    self.model.model.half()  # å¯ç”¨ FP16 åŠç²¾åº¦
                    logger.info("Model using FP16 half precision")
                logger.info(f"Loaded trained model from {trained_model_path} on device {self.device}")
                self.model_loaded = True
                return
            
            # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            pretrained_paths = [
                os.path.join(project_root, 'yolo11n.pt'),
                os.path.join(project_root, 'yolo11s.pt'),
                os.path.join(project_root, 'yolo11m.pt'),
            ]
            for path in pretrained_paths:
                if os.path.exists(path):
                    self.model = YOLO(path)
                    self.model.to(self.device)  # ç§»åŠ¨åˆ°GPU
                    if self.use_half and self.device != 'cpu':
                        self.model.model.half()  # å¯ç”¨ FP16 åŠç²¾åº¦
                        logger.info("Model using FP16 half precision")
                    logger.info(f"Loaded pretrained model from {path} on device {self.device}")
                    self.model_loaded = True
                    return
            
            logger.warning("No YOLO model found, using demo mode")
            self.model_loaded = False
                
        except ImportError:
            logger.error("ultralytics not installed, using demo mode")
            self.model_loaded = False
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            self.model_loaded = False
    
    def _load_device_model(self):
        """åŠ è½½ç”µå­è®¾å¤‡æ£€æµ‹æ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒçš„COCOæ¨¡å‹ï¼‰"""
        try:
            from ultralytics import YOLO
            
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            
            # ä½¿ç”¨é¢„è®­ç»ƒçš„YOLOv11æ¨¡å‹æ£€æµ‹ç”µå­è®¾å¤‡
            pretrained_paths = [
                os.path.join(project_root, 'yolo11n.pt'),
                os.path.join(project_root, 'yolo11s.pt'),
                os.path.join(project_root, 'yolo11m.pt'),
            ]
            
            for path in pretrained_paths:
                if os.path.exists(path):
                    self.device_model = YOLO(path)
                    self.device_model.to(self.device)  # ç§»åŠ¨åˆ°GPU
                    if self.use_half and self.device != 'cpu':
                        self.device_model.model.half()  # å¯ç”¨ FP16 åŠç²¾åº¦
                    logger.info(f"Loaded device detection model from {path} on device {self.device}")
                    self.device_model_loaded = True
                    return
            
            logger.warning("No pretrained model found for device detection")
            self.device_model_loaded = False
            
        except Exception as e:
            logger.error(f"Failed to load device model: {e}")
            self.device_model_loaded = False
    
    def _load_face_detector(self):
        """åŠ è½½äººè„¸æ£€æµ‹å™¨ï¼ˆç”¨äºä½å¤´æ£€æµ‹ï¼‰"""
        try:
            # ä½¿ç”¨OpenCVçš„Haarçº§è”åˆ†ç±»å™¨æ£€æµ‹äººè„¸
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            
            # ä¹ŸåŠ è½½ä¾§è„¸æ£€æµ‹å™¨
            profile_path = cv2.data.haarcascades + 'haarcascade_profileface.xml'
            self.profile_cascade = cv2.CascadeClassifier(profile_path)
            
            logger.info("Face detector loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load face detector: {e}")
            self.face_cascade = None
            self.profile_cascade = None
    
    def _detect_head_down(self, image: np.ndarray, person_boxes: List[List[float]], existing_detections: List = None) -> List[Dict]:
        """
        æ”¹è¿›çš„ä½å¤´æ£€æµ‹ç®—æ³•
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. åªæ£€æµ‹è¿‘è·ç¦»çš„å¤§ç›®æ ‡ï¼ˆé¿å…è¿œå¤„å°ç›®æ ‡è¯¯æ£€ï¼‰
        2. æ£€æµ‹æ•´ä¸ªäººä½“ä¸ŠåŠéƒ¨åˆ†åŒºåŸŸçš„äººè„¸
        3. å¦‚æœåœ¨æ•´ä¸ªåŒºåŸŸéƒ½æ£€æµ‹ä¸åˆ°äººè„¸ï¼Œæ‰åˆ¤å®šä¸ºä½å¤´
        4. å¢åŠ æ›´å¤šè¿‡æ»¤æ¡ä»¶å‡å°‘è¯¯æ£€
        
        Args:
            image: å›¾åƒ
            person_boxes: äººä½“è¾¹ç•Œæ¡†åˆ—è¡¨ [[x1,y1,x2,y2], ...]
            existing_detections: å·²æœ‰çš„æ£€æµ‹ç»“æœï¼Œç”¨äºé¿å…ä¸å…¶ä»–è¡Œä¸ºå†²çª
            
        Returns:
            ä½å¤´æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        head_down_detections = []
        
        if self.face_cascade is None:
            return head_down_detections
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        h, w = image.shape[:2]
        
        # è·å–å·²æ£€æµ‹åˆ°çš„è¡Œä¸ºåŒºåŸŸ
        existing_boxes = []
        if existing_detections:
            for det in existing_detections:
                if det.class_id in [0, 2, 3, 4, 5, 6]:
                    existing_boxes.append(det.bbox)
        
        for person_box in person_boxes:
            x1, y1, x2, y2 = [int(v) for v in person_box]
            
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(w, x2)
            y2 = min(h, y2)
            
            if x2 <= x1 or y2 <= y1:
                continue
            
            person_height = y2 - y1
            person_width = x2 - x1
            
            # ä¸¥æ ¼è¿‡æ»¤ï¼šåªæ£€æµ‹å å›¾åƒ30%ä»¥ä¸Šçš„å¤§ç›®æ ‡
            if person_height < h * 0.3:
                continue
            
            if person_height < 200 or person_width < 100:
                continue
            
            aspect_ratio = person_width / person_height
            if aspect_ratio > 1.2 or aspect_ratio < 0.25:
                continue
            
            # æ£€æŸ¥ä¸å·²æ£€æµ‹è¡Œä¸ºçš„é‡å 
            skip_person = False
            for eb in existing_boxes:
                ex1, ey1, ex2, ey2 = [int(v) for v in eb]
                inter_x1 = max(x1, ex1)
                inter_y1 = max(y1, ey1)
                inter_x2 = min(x2, ex2)
                inter_y2 = min(y2, ey2)
                
                if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                    person_area = (x2 - x1) * (y2 - y1)
                    if inter_area / person_area > 0.15:
                        skip_person = True
                        break
            
            if skip_person:
                continue
            
            # æ£€æµ‹äººä½“ä¸ŠåŠéƒ¨åˆ†çš„äººè„¸
            head_y2 = y1 + int(person_height * 0.5)
            person_region = gray[y1:head_y2, x1:x2]
            
            if person_region.size == 0:
                continue
            
            # å®½æ¾å‚æ•°æ£€æµ‹äººè„¸
            faces = self.face_cascade.detectMultiScale(
                person_region,
                scaleFactor=1.1,
                minNeighbors=3,
                minSize=(20, 20),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            if len(faces) > 0:
                continue
            
            # æ£€æµ‹ä¾§è„¸
            if self.profile_cascade is not None:
                profiles = self.profile_cascade.detectMultiScale(
                    person_region,
                    scaleFactor=1.1,
                    minNeighbors=3,
                    minSize=(20, 20),
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                
                if len(profiles) > 0:
                    continue
                
                flipped = cv2.flip(person_region, 1)
                profiles_flip = self.profile_cascade.detectMultiScale(
                    flipped,
                    scaleFactor=1.1,
                    minNeighbors=3,
                    minSize=(20, 20),
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                
                if len(profiles_flip) > 0:
                    continue
            
            # åˆ¤å®šä¸ºä½å¤´
            confidence = 0.6 + (person_height / h) * 0.2
            confidence = min(0.85, confidence)
            
            head_down_detections.append({
                'bbox': [x1, y1, x2, y1 + int(person_height * 0.45)],
                'confidence': confidence,
                'reason': 'no_face_detected'
            })
        
        return head_down_detections
    
    def detect_image(self, image: np.ndarray) -> Tuple[np.ndarray, DetectionResult]:
        """
        æ£€æµ‹å•å¼ å›¾ç‰‡
        
        Args:
            image: OpenCVæ ¼å¼çš„å›¾ç‰‡ (BGR)
            
        Returns:
            (æ ‡æ³¨åçš„å›¾ç‰‡, æ£€æµ‹ç»“æœ)
        """
        detections = []
        device_detections = []  # ç”µå­è®¾å¤‡æ£€æµ‹ç»“æœ
        person_boxes = []  # äººä½“è¾¹ç•Œæ¡†ï¼ˆç”¨äºä½å¤´æ£€æµ‹ï¼‰
        behavior_summary = {info['cn_name']: 0 for info in BEHAVIOR_CLASSES.values()}
        alert_summary = {level['cn_name']: 0 for level in ALERT_LEVELS.values()}
        
        # 1. å…ˆæ£€æµ‹ç”µå­è®¾å¤‡å’Œäººä½“
        if self.device_model is not None and self.device_model_loaded:
            try:
                device_results = self.device_model(
                    image, 
                    conf=0.3, 
                    iou=self.iou_threshold, 
                    imgsz=self.imgsz,
                    half=self.use_half,
                    verbose=False
                )
                for result in device_results:
                    boxes = result.boxes
                    if boxes is not None and len(boxes) > 0:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            # æ£€æµ‹ç”µå­è®¾å¤‡
                            if cls_id in self.ELECTRONIC_DEVICE_CLASSES:
                                device_name = self.ELECTRONIC_DEVICE_CLASSES[cls_id]
                                device_detections.append({
                                    'class_id': cls_id,
                                    'name': device_name,
                                    'confidence': conf,
                                    'bbox': xyxy
                                })
                            
                            # æ£€æµ‹äººä½“ï¼ˆç”¨äºä½å¤´æ£€æµ‹ï¼‰
                            if cls_id == self.PERSON_CLASS_ID and conf > 0.4:
                                person_boxes.append(xyxy)
                                
            except Exception as e:
                logger.error(f"Device detection error: {e}")
        
        # 2. è¡Œä¸ºæ£€æµ‹
        if self.model is not None and self.model_loaded:
            try:
                # è¿è¡ŒYOLOæ£€æµ‹ï¼ˆä½¿ç”¨ä¼˜åŒ–å‚æ•°ï¼‰
                results = self.model(
                    image, 
                    conf=self.confidence_threshold, 
                    iou=self.iou_threshold, 
                    imgsz=self.imgsz,
                    half=self.use_half,
                    verbose=False
                )
                
                for result in results:
                    boxes = result.boxes
                    if boxes is not None and len(boxes) > 0:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            # è·å–ç±»åˆ«ä¿¡æ¯
                            if cls_id in BEHAVIOR_CLASSES:
                                class_info = BEHAVIOR_CLASSES[cls_id]
                            else:
                                logger.warning(f"Unknown class_id: {cls_id}")
                                continue  # è·³è¿‡æœªçŸ¥ç±»åˆ«
                            
                            # è·å–é¢„è­¦çº§åˆ«
                            alert_level = 0
                            for level, level_info in ALERT_LEVELS.items():
                                if cls_id in level_info['classes']:
                                    alert_level = level
                                    break
                            
                            detection = Detection(
                                class_id=cls_id,
                                class_name=class_info['name'],
                                class_name_cn=class_info['cn_name'],
                                confidence=round(conf, 3),
                                bbox=[round(v, 1) for v in xyxy],
                                behavior_type=class_info['type'],
                                alert_level=alert_level
                            )
                            detections.append(detection)
                            behavior_summary[class_info['cn_name']] += 1
                            alert_summary[ALERT_LEVELS[alert_level]['cn_name']] += 1
                            
            except Exception as e:
                logger.error(f"Detection error: {e}", exc_info=True)
                detections, behavior_summary, alert_summary = self._generate_demo_detections(image)
        else:
            # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ
            detections, behavior_summary, alert_summary = self._generate_demo_detections(image)
        
        # 3. å¦‚æœæ£€æµ‹åˆ°ç”µå­è®¾å¤‡ä½†æ²¡æœ‰æ£€æµ‹åˆ°"ä½¿ç”¨ç”µå­è®¾å¤‡"è¡Œä¸ºï¼Œæ·»åŠ è¯¥è¡Œä¸º
        if device_detections and not any(d.class_id == 5 for d in detections):
            for device in device_detections:
                # ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„ç”µå­è®¾å¤‡åˆ›å»ºä¸€ä¸ª"ä½¿ç”¨ç”µå­è®¾å¤‡"çš„æ£€æµ‹ç»“æœ
                device_class_info = BEHAVIOR_CLASSES[5]  # using_electronic_devices
                detection = Detection(
                    class_id=5,
                    class_name=device_class_info['name'],
                    class_name_cn=f"{device_class_info['cn_name']}({device['name']})",
                    confidence=round(device['confidence'], 3),
                    bbox=[round(v, 1) for v in device['bbox']],
                    behavior_type=device_class_info['type'],
                    alert_level=3  # ä¸¥é‡é¢„è­¦
                )
                detections.append(detection)
                behavior_summary[device_class_info['cn_name']] += 1
                alert_summary[ALERT_LEVELS[3]['cn_name']] += 1
        
        # 4. ä½å¤´æ£€æµ‹ï¼ˆä¼ å…¥å·²æœ‰æ£€æµ‹ç»“æœä»¥é¿å…ä¸ä¹¦å†™è¡Œä¸ºå†²çªï¼‰
        if person_boxes:
            head_down_results = self._detect_head_down(image, person_boxes, detections)
            for hd in head_down_results:
                head_down_class_info = BEHAVIOR_CLASSES[7]  # head_down
                detection = Detection(
                    class_id=7,
                    class_name=head_down_class_info['name'],
                    class_name_cn=head_down_class_info['cn_name'],
                    confidence=round(hd['confidence'], 3),
                    bbox=[round(v, 1) for v in hd['bbox']],
                    behavior_type=head_down_class_info['type'],
                    alert_level=1  # è½»åº¦é¢„è­¦
                )
                detections.append(detection)
                behavior_summary[head_down_class_info['cn_name']] += 1
                alert_summary[ALERT_LEVELS[1]['cn_name']] += 1
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        annotated_image = self._draw_detections(image.copy(), detections, device_detections)
        
        # ç»Ÿè®¡ç»“æœ
        warning_count = sum(1 for d in detections if d.behavior_type == 'warning')
        normal_count = sum(1 for d in detections if d.behavior_type == 'normal')
        
        # æ›´æ–°è¡Œä¸ºæ—¶é—´ç»Ÿè®¡
        self.time_tracker.update(detections)
        
        result = DetectionResult(
            detections=detections,
            total_count=len(detections),
            warning_count=warning_count,
            normal_count=normal_count,
            behavior_summary=behavior_summary,
            alert_summary=alert_summary,
            timestamp=datetime.now().isoformat(),
            behavior_duration=self.time_tracker.get_duration()
        )
        
        return annotated_image, result
    
    def _generate_demo_detections(self, image: np.ndarray) -> Tuple[List[Detection], Dict[str, int], Dict[str, int]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ£€æµ‹ç»“æœç”¨äºæ¼”ç¤º"""
        import random
        
        h, w = image.shape[:2]
        detections = []
        behavior_summary = {info['cn_name']: 0 for info in BEHAVIOR_CLASSES.values()}
        alert_summary = {level['cn_name']: 0 for level in ALERT_LEVELS.values()}
        
        # ç”Ÿæˆ3-8ä¸ªéšæœºæ£€æµ‹æ¡†
        num_detections = random.randint(3, 8)
        
        for i in range(num_detections):
            # éšæœºé€‰æ‹©è¡Œä¸ºç±»åˆ«
            class_id = random.choice(list(BEHAVIOR_CLASSES.keys()))
            class_info = BEHAVIOR_CLASSES[class_id]
            
            # ç”Ÿæˆéšæœºè¾¹ç•Œæ¡†
            box_w = random.randint(80, 150)
            box_h = random.randint(100, 200)
            x1 = random.randint(50, max(51, w - box_w - 50))
            y1 = random.randint(50, max(51, h - box_h - 50))
            x2 = x1 + box_w
            y2 = y1 + box_h
            
            # è·å–é¢„è­¦çº§åˆ«
            alert_level = 0
            for level, level_info in ALERT_LEVELS.items():
                if class_id in level_info['classes']:
                    alert_level = level
                    break
            
            detection = Detection(
                class_id=class_id,
                class_name=class_info['name'],
                class_name_cn=class_info['cn_name'],
                confidence=round(random.uniform(0.6, 0.95), 3),
                bbox=[x1, y1, x2, y2],
                behavior_type=class_info['type'],
                alert_level=alert_level
            )
            detections.append(detection)
            behavior_summary[class_info['cn_name']] += 1
            alert_summary[ALERT_LEVELS[alert_level]['cn_name']] += 1
        
        return detections, behavior_summary, alert_summary
    
    def _draw_detections(self, image: np.ndarray, detections: List[Detection], device_detections: List[Dict] = None) -> np.ndarray:
        """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†ï¼ˆæ”¯æŒä¸­æ–‡æ ‡ç­¾ï¼‰"""
        # è½¬æ¢ä¸ºPILå›¾åƒä»¥æ”¯æŒä¸­æ–‡
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(image_rgb)
        draw = ImageDraw.Draw(pil_image)
        
        # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
        font = None
        font_size = 20
        try:
            # Windows ç³»ç»Ÿå­—ä½“è·¯å¾„
            font_paths = [
                "C:/Windows/Fonts/msyh.ttc",  # å¾®è½¯é›…é»‘
                "C:/Windows/Fonts/simhei.ttf",  # é»‘ä½“
                "C:/Windows/Fonts/simsun.ttc",  # å®‹ä½“
                "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",  # Linux
                "/System/Library/Fonts/PingFang.ttc",  # macOS
            ]
            for font_path in font_paths:
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, font_size)
                    break
            if font is None:
                font = ImageFont.load_default()
        except Exception as e:
            logger.warning(f"Failed to load font: {e}")
            font = ImageFont.load_default()
        
        # ç»˜åˆ¶ç”µå­è®¾å¤‡æ£€æµ‹æ¡†ï¼ˆè“è‰²ï¼‰
        if device_detections:
            for device in device_detections:
                x1, y1, x2, y2 = [int(v) for v in device['bbox']]
                device_color = (0, 100, 255)  # è“è‰²
                draw.rectangle([x1, y1, x2, y2], outline=device_color, width=2)
                
                # ç»˜åˆ¶è®¾å¤‡æ ‡ç­¾
                device_label = f"ğŸ“±{device['name']} {device['confidence']:.2f}"
                try:
                    bbox = draw.textbbox((0, 0), device_label, font=font)
                    label_w = bbox[2] - bbox[0]
                    label_h = bbox[3] - bbox[1]
                except:
                    label_w, label_h = len(device_label) * 10, 20
                
                label_y = max(0, y1 - label_h - 6)
                draw.rectangle([x1, label_y, x1 + label_w + 10, y1], fill=device_color)
                draw.text((x1 + 5, label_y + 2), device_label, fill=(255, 255, 255), font=font)
        
        # ç»˜åˆ¶è¡Œä¸ºæ£€æµ‹æ¡†
        for det in detections:
            x1, y1, x2, y2 = [int(v) for v in det.bbox]
            
            # è·å–é¢œè‰² (RGB)
            color_rgb = BEHAVIOR_CLASSES.get(det.class_id, {}).get('color', (0, 255, 0))
            
            # æ ¹æ®é¢„è­¦çº§åˆ«è°ƒæ•´è¾¹æ¡†ç²—ç»†
            thickness = 2 if det.alert_level == 0 else 3
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            draw.rectangle([x1, y1, x2, y2], outline=color_rgb, width=thickness)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{det.class_name_cn} {det.confidence:.2f}"
            
            # è·å–æ–‡å­—å¤§å°
            try:
                bbox = draw.textbbox((0, 0), label, font=font)
                label_w = bbox[2] - bbox[0]
                label_h = bbox[3] - bbox[1]
            except:
                label_w, label_h = len(label) * 10, 20
            
            # æ ‡ç­¾èƒŒæ™¯
            label_y = max(0, y1 - label_h - 6)
            draw.rectangle([x1, label_y, x1 + label_w + 10, y1], fill=color_rgb)
            
            # ç»˜åˆ¶æ ‡ç­¾æ–‡å­—ï¼ˆç™½è‰²ï¼‰
            draw.text((x1 + 5, label_y + 2), label, fill=(255, 255, 255), font=font)
            
            # å¦‚æœæ˜¯é¢„è­¦è¡Œä¸ºï¼Œæ·»åŠ è­¦å‘Šæ ‡è®°
            if det.behavior_type == 'warning':
                # ç»˜åˆ¶è­¦å‘Šåœ†ç‚¹
                warn_x, warn_y = x2 - 15, y1 + 15
                draw.ellipse([warn_x - 10, warn_y - 10, warn_x + 10, warn_y + 10], fill=(255, 0, 0))
                draw.text((warn_x - 5, warn_y - 8), "!", fill=(255, 255, 255), font=font)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        warning_count = sum(1 for d in detections if d.behavior_type == 'warning')
        device_count = len(device_detections) if device_detections else 0
        stats_text = f"æ£€æµ‹: {len(detections)} | é¢„è­¦: {warning_count} | ç”µå­è®¾å¤‡: {device_count}"
        draw.text((10, 10), stats_text, fill=(0, 255, 0), font=font)
        
        # è½¬æ¢å›OpenCVæ ¼å¼
        result_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        return result_image
    
    def detect_base64(self, base64_image: str) -> Tuple[str, DetectionResult]:
        """
        æ£€æµ‹Base64ç¼–ç çš„å›¾ç‰‡
        
        Args:
            base64_image: Base64ç¼–ç çš„å›¾ç‰‡å­—ç¬¦ä¸²
            
        Returns:
            (Base64ç¼–ç çš„æ ‡æ³¨å›¾ç‰‡, æ£€æµ‹ç»“æœ)
        """
        # è§£ç Base64å›¾ç‰‡
        if ',' in base64_image:
            base64_image = base64_image.split(',')[1]
        
        image_data = base64.b64decode(base64_image)
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise ValueError("Invalid image data")
        
        # æ£€æµ‹
        annotated_image, result = self.detect_image(image)
        
        # ç¼–ç ä¸ºBase64
        _, buffer = cv2.imencode('.jpg', annotated_image, [cv2.IMWRITE_JPEG_QUALITY, 90])
        annotated_base64 = base64.b64encode(buffer).decode('utf-8')
        
        return f"data:image/jpeg;base64,{annotated_base64}", result
    
    def detect_base64_fast(self, base64_image: str, skip_detection: bool = False) -> Tuple[str, DetectionResult]:
        """
        å¿«é€Ÿæ£€æµ‹Base64ç¼–ç çš„å›¾ç‰‡ï¼ˆæ”¯æŒè·³å¸§ï¼‰
        
        Args:
            base64_image: Base64ç¼–ç çš„å›¾ç‰‡å­—ç¬¦ä¸²
            skip_detection: æ˜¯å¦è·³è¿‡æ£€æµ‹ï¼ˆä½¿ç”¨ç¼“å­˜ç»“æœï¼‰
            
        Returns:
            (Base64ç¼–ç çš„æ ‡æ³¨å›¾ç‰‡, æ£€æµ‹ç»“æœ)
        """
        # æ›´æ–°FPSè®¡æ•°
        self._fps_counter.tick()
        
        # è§£ç Base64å›¾ç‰‡
        if ',' in base64_image:
            base64_image = base64_image.split(',')[1]
        
        image_data = base64.b64decode(base64_image)
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise ValueError("Invalid image data")
        
        # å¸§è®¡æ•°
        self._frame_count_detection += 1
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œæ£€æµ‹
        should_detect = (self._frame_count_detection % (self._frame_skip + 1) == 0) or self._last_result is None
        
        if should_detect and not skip_detection:
            # æ‰§è¡Œå¿«é€Ÿæ£€æµ‹ï¼ˆç¦ç”¨ä½å¤´æ£€æµ‹ä»¥æé«˜æ€§èƒ½ï¼‰
            with self._detection_lock:
                annotated_image, result = self.detect_image_fast(image)
                self._last_result = result
                self._last_annotated_image = annotated_image
        else:
            # ä½¿ç”¨ç¼“å­˜çš„æ£€æµ‹ç»“æœï¼Œä½†åœ¨å½“å‰å¸§ä¸Šç»˜åˆ¶
            if self._last_result is not None:
                annotated_image = self._draw_detections_simple(image.copy(), self._last_result.detections)
                result = self._last_result
            else:
                annotated_image = image
                result = DetectionResult(
                    detections=[],
                    total_count=0,
                    warning_count=0,
                    normal_count=0,
                    behavior_summary={info['cn_name']: 0 for info in BEHAVIOR_CLASSES.values()},
                    alert_summary={level['cn_name']: 0 for level in ALERT_LEVELS.values()},
                    timestamp=datetime.now().isoformat()
                )
        
        # åœ¨å›¾åƒä¸Šæ·»åŠ FPSä¿¡æ¯ï¼ˆä½¿ç”¨OpenCVï¼Œæ›´å¿«ï¼‰
        fps = self._fps_counter.get_fps()
        cv2.putText(annotated_image, f"FPS: {fps:.1f}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        
        # ç¼–ç ä¸ºBase64ï¼ˆé™ä½è´¨é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
        _, buffer = cv2.imencode('.jpg', annotated_image, [cv2.IMWRITE_JPEG_QUALITY, 70])
        annotated_base64 = base64.b64encode(buffer).decode('utf-8')
        
        return f"data:image/jpeg;base64,{annotated_base64}", result
    
    def detect_image_fast(self, image: np.ndarray) -> Tuple[np.ndarray, DetectionResult]:
        """
        å¿«é€Ÿæ£€æµ‹å•å¼ å›¾ç‰‡ï¼ˆåŒ…å«ä½å¤´æ£€æµ‹å’Œè®¾å¤‡æ£€æµ‹ï¼‰
        
        Args:
            image: OpenCVæ ¼å¼çš„å›¾ç‰‡ (BGR)
            
        Returns:
            (æ ‡æ³¨åçš„å›¾ç‰‡, æ£€æµ‹ç»“æœ)
        """
        detections = []
        device_detections = []
        person_boxes = []
        behavior_summary = {info['cn_name']: 0 for info in BEHAVIOR_CLASSES.values()}
        alert_summary = {level['cn_name']: 0 for level in ALERT_LEVELS.values()}
        
        # 1. ç”µå­è®¾å¤‡å’Œäººä½“æ£€æµ‹
        if self.device_model is not None and self.device_model_loaded:
            try:
                device_results = self.device_model(
                    image, 
                    conf=0.3, 
                    iou=self.iou_threshold, 
                    imgsz=self.imgsz,
                    half=self.use_half,
                    verbose=False
                )
                for result in device_results:
                    boxes = result.boxes
                    if boxes is not None and len(boxes) > 0:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            # æ£€æµ‹ç”µå­è®¾å¤‡
                            if cls_id in self.ELECTRONIC_DEVICE_CLASSES:
                                device_name = self.ELECTRONIC_DEVICE_CLASSES[cls_id]
                                device_detections.append({
                                    'class_id': cls_id,
                                    'name': device_name,
                                    'confidence': conf,
                                    'bbox': xyxy
                                })
                            
                            # æ£€æµ‹äººä½“ï¼ˆç”¨äºä½å¤´æ£€æµ‹ï¼‰
                            if cls_id == self.PERSON_CLASS_ID and conf > 0.4:
                                person_boxes.append(xyxy)
            except Exception as e:
                logger.error(f"Device detection error: {e}")
        
        # 2. è¡Œä¸ºæ£€æµ‹
        if self.model is not None and self.model_loaded:
            try:
                results = self.model(
                    image, 
                    conf=self.confidence_threshold, 
                    iou=self.iou_threshold, 
                    imgsz=self.imgsz,
                    half=self.use_half,
                    verbose=False
                )
                
                for result in results:
                    boxes = result.boxes
                    if boxes is not None and len(boxes) > 0:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            # è·å–ç±»åˆ«ä¿¡æ¯
                            if cls_id in BEHAVIOR_CLASSES:
                                class_info = BEHAVIOR_CLASSES[cls_id]
                            else:
                                continue
                            
                            # è·å–é¢„è­¦çº§åˆ«
                            alert_level = 0
                            for level, level_info in ALERT_LEVELS.items():
                                if cls_id in level_info['classes']:
                                    alert_level = level
                                    break
                            
                            detection = Detection(
                                class_id=cls_id,
                                class_name=class_info['name'],
                                class_name_cn=class_info['cn_name'],
                                confidence=round(conf, 3),
                                bbox=[round(v, 1) for v in xyxy],
                                behavior_type=class_info['type'],
                                alert_level=alert_level
                            )
                            detections.append(detection)
                            behavior_summary[class_info['cn_name']] += 1
                            alert_summary[ALERT_LEVELS[alert_level]['cn_name']] += 1
                            
            except Exception as e:
                logger.error(f"Fast detection error: {e}")
        
        # 3. æ·»åŠ ç”µå­è®¾å¤‡æ£€æµ‹ç»“æœ
        if device_detections and not any(d.class_id == 5 for d in detections):
            for device in device_detections:
                device_class_info = BEHAVIOR_CLASSES[5]
                detection = Detection(
                    class_id=5,
                    class_name=device_class_info['name'],
                    class_name_cn=f"{device_class_info['cn_name']}({device['name']})",
                    confidence=round(device['confidence'], 3),
                    bbox=[round(v, 1) for v in device['bbox']],
                    behavior_type=device_class_info['type'],
                    alert_level=3
                )
                detections.append(detection)
                behavior_summary[device_class_info['cn_name']] += 1
                alert_summary[ALERT_LEVELS[3]['cn_name']] += 1
        
        # 4. ä½å¤´æ£€æµ‹
        if person_boxes:
            head_down_results = self._detect_head_down(image, person_boxes, detections)
            for hd in head_down_results:
                head_down_class_info = BEHAVIOR_CLASSES[7]
                detection = Detection(
                    class_id=7,
                    class_name=head_down_class_info['name'],
                    class_name_cn=head_down_class_info['cn_name'],
                    confidence=round(hd['confidence'], 3),
                    bbox=[round(v, 1) for v in hd['bbox']],
                    behavior_type=head_down_class_info['type'],
                    alert_level=1
                )
                detections.append(detection)
                behavior_summary[head_down_class_info['cn_name']] += 1
                alert_summary[ALERT_LEVELS[1]['cn_name']] += 1
        
        # ä½¿ç”¨ç®€åŒ–çš„ç»˜åˆ¶æ–¹æ³•
        annotated_image = self._draw_detections_simple(image.copy(), detections, device_detections)
        
        # ç»Ÿè®¡ç»“æœ
        warning_count = sum(1 for d in detections if d.behavior_type == 'warning')
        normal_count = sum(1 for d in detections if d.behavior_type == 'normal')
        
        # æ›´æ–°è¡Œä¸ºæ—¶é—´ç»Ÿè®¡
        self.time_tracker.update(detections)
        
        result = DetectionResult(
            detections=detections,
            total_count=len(detections),
            warning_count=warning_count,
            normal_count=normal_count,
            behavior_summary=behavior_summary,
            alert_summary=alert_summary,
            timestamp=datetime.now().isoformat(),
            behavior_duration=self.time_tracker.get_duration()
        )
        
        return annotated_image, result
    
    def _draw_detections_simple(self, image: np.ndarray, detections: List[Detection], device_detections: List[Dict] = None) -> np.ndarray:
        """ç®€åŒ–çš„æ£€æµ‹æ¡†ç»˜åˆ¶ï¼ˆä½¿ç”¨OpenCVï¼Œæ›´å¿«ï¼‰"""
        # ç»˜åˆ¶ç”µå­è®¾å¤‡æ£€æµ‹æ¡†ï¼ˆè“è‰²ï¼‰
        if device_detections:
            for device in device_detections:
                x1, y1, x2, y2 = [int(v) for v in device['bbox']]
                cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 0), 2)
                label = f"phone {device['confidence']:.2f}"
                cv2.putText(image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 0), 2)
        
        for det in detections:
            x1, y1, x2, y2 = [int(v) for v in det.bbox]
            
            # è·å–é¢œè‰² (BGR for OpenCV)
            color_rgb = BEHAVIOR_CLASSES.get(det.class_id, {}).get('color', (0, 255, 0))
            color_bgr = (color_rgb[2], color_rgb[1], color_rgb[0])
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            thickness = 3 if det.behavior_type == 'warning' else 2
            cv2.rectangle(image, (x1, y1), (x2, y2), color_bgr, thickness)
            
            # ç»˜åˆ¶æ ‡ç­¾ï¼ˆè‹±æ–‡ï¼Œé¿å…ä¸­æ–‡å­—ä½“åŠ è½½ï¼‰
            label = f"{det.class_name} {det.confidence:.2f}"
            font_scale = 0.6
            font_thickness = 2
            (label_w, label_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
            
            # æ ‡ç­¾èƒŒæ™¯
            label_y = max(label_h + 10, y1)
            cv2.rectangle(image, (x1, label_y - label_h - 10), (x1 + label_w + 10, label_y), color_bgr, -1)
            
            # æ ‡ç­¾æ–‡å­—
            cv2.putText(image, label, (x1 + 5, label_y - 5), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)
            
            # é¢„è­¦æ ‡è®°
            if det.behavior_type == 'warning':
                cv2.circle(image, (x2 - 15, y1 + 15), 10, (0, 0, 255), -1)
                cv2.putText(image, "!", (x2 - 20, y1 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ç»Ÿè®¡ä¿¡æ¯
        warning_count = sum(1 for d in detections if d.behavior_type == 'warning')
        cv2.rectangle(image, (5, 5), (220, 35), (0, 0, 0), -1)
        cv2.putText(image, f"Detect: {len(detections)} | Warn: {warning_count}", (10, 28), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        return image
    
    def set_frame_skip(self, skip: int):
        """è®¾ç½®è·³å¸§æ•°ï¼ˆ0è¡¨ç¤ºä¸è·³å¸§ï¼‰"""
        self._frame_skip = max(0, min(10, skip))
    
    def get_fps(self) -> float:
        """è·å–å½“å‰FPS"""
        return self._fps_counter.get_fps()
    
    def set_confidence_threshold(self, threshold: float):
        """è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼"""
        self.confidence_threshold = max(0.1, min(0.9, threshold))
    
    def set_iou_threshold(self, threshold: float):
        """è®¾ç½®IOUé˜ˆå€¼"""
        self.iou_threshold = max(0.1, min(0.9, threshold))
    
    def detect_batch(self, images: List[np.ndarray], batch_size: int = 4) -> List[DetectionResult]:
        """
        æ‰¹é‡æ£€æµ‹å¤šå¼ å›¾ç‰‡ï¼ˆGPU ä¼˜åŒ–ï¼‰
        
        Args:
            images: å›¾ç‰‡åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œæ ¹æ® GPU å†…å­˜è°ƒæ•´
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        if not images:
            return []
        
        results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i + batch_size]
            batch_results = self._process_batch(batch_images)
            results.extend(batch_results)
        
        return results
    
    def _process_batch(self, batch_images: List[np.ndarray]) -> List[DetectionResult]:
        """
        å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡
        
        Args:
            batch_images: æ‰¹æ¬¡å›¾ç‰‡åˆ—è¡¨
            
        Returns:
            æ‰¹æ¬¡æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        batch_results = []
        
        if self.model is not None and self.model_loaded:
            try:
                # å¯¹äº YOLOï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦é€å¼ å¤„ç†ï¼Œä½†å¯ä»¥ä¼˜åŒ–å…¶ä»–éƒ¨åˆ†
                for image in batch_images:
                    # ä½¿ç”¨å¿«é€Ÿæ£€æµ‹æ–¹æ³•
                    _, result = self.detect_image_fast(image)
                    batch_results.append(result)
                    
            except Exception as e:
                logger.error(f"Batch detection error: {e}")
                # é™çº§åˆ°å•å¼ å¤„ç†
                for image in batch_images:
                    try:
                        _, result = self.detect_image(image)
                        batch_results.append(result)
                    except Exception as e2:
                        logger.error(f"Single image detection error: {e2}")
                        # åˆ›å»ºç©ºç»“æœ
                        empty_result = DetectionResult(
                            detections=[],
                            total_count=0,
                            warning_count=0,
                            normal_count=0,
                            behavior_summary={info['cn_name']: 0 for info in BEHAVIOR_CLASSES.values()},
                            alert_summary={level['cn_name']: 0 for level in ALERT_LEVELS.values()},
                            timestamp=datetime.now().isoformat()
                        )
                        batch_results.append(empty_result)
        else:
            # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ
            for image in batch_images:
                detections, behavior_summary, alert_summary = self._generate_demo_detections(image)
                result = DetectionResult(
                    detections=detections,
                    total_count=len(detections),
                    warning_count=sum(1 for d in detections if d.behavior_type == 'warning'),
                    normal_count=sum(1 for d in detections if d.behavior_type == 'normal'),
                    behavior_summary=behavior_summary,
                    alert_summary=alert_summary,
                    timestamp=datetime.now().isoformat()
                )
                batch_results.append(result)
        
        return batch_results
    
    def detect_video_optimized(self, video_path: str, frame_skip: int = 5, batch_size: int = 4, 
                              progress_callback=None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„è§†é¢‘æ£€æµ‹æ–¹æ³•ï¼ˆGPU åŠ é€Ÿï¼‰
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            frame_skip: è·³å¸§æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ£€æµ‹ç»“æœç»Ÿè®¡
        """
        import cv2
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {total_frames} å¸§, FPS: {fps}")
        
        # æ”¶é›†éœ€è¦å¤„ç†çš„å¸§
        frames_to_process = []
        frame_indices = []
        frame_count = 0
        processed_count = 0
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†å¸§æ•°æ®
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # è·³å¸§å¤„ç†
            if frame_count % (frame_skip + 1) != 0:
                continue
            
            frames_to_process.append(frame.copy())
            frame_indices.append(frame_count)
            processed_count += 1
            
            # å½“æ”¶é›†åˆ°è¶³å¤Ÿçš„å¸§æˆ–åˆ°è¾¾è§†é¢‘æœ«å°¾æ—¶ï¼Œè¿›è¡Œæ‰¹å¤„ç†
            if len(frames_to_process) >= batch_size:
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_results = self.detect_batch(frames_to_process, batch_size)
                
                # æ¸…ç©ºç¼“å­˜
                frames_to_process = []
                frame_indices = []
                
                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress = processed_count / (total_frames // (frame_skip + 1))
                    progress_callback(progress, processed_count)
        
        # å¤„ç†å‰©ä½™çš„å¸§
        if frames_to_process:
            batch_results = self.detect_batch(frames_to_process, len(frames_to_process))
        
        cap.release()
        
        # ç»Ÿè®¡ç»“æœ
        total_detections = 0
        warning_count = 0
        behavior_totals = {}
        
        logger.info(f"è§†é¢‘å¤„ç†å®Œæˆ: å¤„ç†äº† {processed_count} å¸§")
        
        return {
            'total_frames': total_frames,
            'processed_frames': processed_count,
            'video_fps': fps,
            'total_detections': total_detections,
            'warning_count': warning_count,
            'behavior_summary': behavior_totals,
            'processing_time': 0  # å¯ä»¥æ·»åŠ è®¡æ—¶
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        gpu_info = {}
        if self.device != 'cpu':
            try:
                import torch
                gpu_info = {
                    'gpu_name': torch.cuda.get_device_name(0),
                    'gpu_memory_total': f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB",
                    'gpu_memory_allocated': f"{torch.cuda.memory_allocated(0) / 1024**3:.2f}GB",
                    'gpu_memory_cached': f"{torch.cuda.memory_reserved(0) / 1024**3:.2f}GB",
                }
            except:
                pass
        
        return {
            'model_loaded': self.model_loaded,
            'device': self.device,
            'using_gpu': self.device != 'cpu',
            'use_half': self.use_half,
            'imgsz': self.imgsz,
            'confidence_threshold': self.confidence_threshold,
            'iou_threshold': self.iou_threshold,
            'num_classes': len(BEHAVIOR_CLASSES),
            'classes': [{'id': k, **v} for k, v in BEHAVIOR_CLASSES.items()],
            **gpu_info
        }
    
    def set_imgsz(self, imgsz: int):
        """è®¾ç½®æ¨ç†å›¾åƒå°ºå¯¸ï¼ˆå½±å“ GPU åˆ©ç”¨ç‡ï¼‰"""
        self.imgsz = max(320, min(1920, imgsz))
        logger.info(f"Image size set to {self.imgsz}")
    
    def set_half_precision(self, use_half: bool):
        """è®¾ç½®æ˜¯å¦ä½¿ç”¨ FP16 åŠç²¾åº¦"""
        if self.device == 'cpu':
            logger.warning("FP16 not supported on CPU")
            return
        self.use_half = use_half
        logger.info(f"Half precision set to {self.use_half}")
    
    def get_time_statistics(self) -> Dict[str, Any]:
        """è·å–è¡Œä¸ºæ—¶é—´ç»Ÿè®¡"""
        return self.time_tracker.get_statistics()
    
    def reset_time_tracker(self):
        """é‡ç½®æ—¶é—´ç»Ÿè®¡"""
        self.time_tracker.reset()
    
    # ==================== æ•°æ®å­˜å‚¨åŠŸèƒ½ ====================
    # æ•´åˆè‡ª model/services/DetectionService.py
    
    def start_session(
        self,
        source_type: str,
        source_path: str = None,
        user_id: int = None,
        schedule_id: int = None
    ) -> int:
        """
        å¼€å§‹æ–°çš„æ£€æµ‹ä¼šè¯
        
        Args:
            source_type: è¾“å…¥æºç±»å‹ (image/video/stream)
            source_path: è¾“å…¥æºè·¯å¾„
            user_id: ç”¨æˆ·ID
            schedule_id: è¯¾å ‚å®‰æ’ID
            
        Returns:
            ä¼šè¯ID
        """
        self._current_session_id = self.data_access.create_session(
            source_type=source_type,
            source_path=source_path,
            user_id=user_id,
            schedule_id=schedule_id
        )
        self._frame_count = 0
        self._record_buffer = []
        self._entry_buffer = []
        
        logger.info(f"Started detection session: {self._current_session_id}")
        return self._current_session_id
    
    def end_session(self, status: str = 'completed') -> Dict[str, Any]:
        """
        ç»“æŸå½“å‰æ£€æµ‹ä¼šè¯
        
        Args:
            status: ä¼šè¯çŠ¶æ€ (completed/failed)
            
        Returns:
            ä¼šè¯ç»Ÿè®¡ä¿¡æ¯
        """
        if self._current_session_id is None:
            logger.warning("No active session to end")
            return {}
        
        # åˆ·æ–°ç¼“å†²åŒº
        self._flush_buffers()
        
        # æ›´æ–°ä¼šè¯
        self.data_access.update_session(
            session_id=self._current_session_id,
            end_time=datetime.now(),
            total_frames=self._frame_count,
            status=status
        )
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.data_access.get_session_statistics(self._current_session_id)
        
        logger.info(f"Ended detection session: {self._current_session_id}, frames: {self._frame_count}")
        
        session_id = self._current_session_id
        self._current_session_id = None
        self._frame_count = 0
        
        return stats
    
    @property
    def current_session_id(self) -> Optional[int]:
        """è·å–å½“å‰ä¼šè¯ID"""
        return self._current_session_id
    
    def save_detection_result(
        self,
        frame_id: int,
        timestamp: float,
        detections: List[Dict[str, Any]],
        alert_triggered: bool = False
    ) -> int:
        """
        ä¿å­˜å•å¸§æ£€æµ‹ç»“æœ
        
        Args:
            frame_id: å¸§ID
            timestamp: æ—¶é—´æˆ³
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªæ£€æµ‹åŒ…å«:
                - bbox: (x1, y1, x2, y2)
                - class_id: ç±»åˆ«ID
                - class_name: ç±»åˆ«åç§°
                - confidence: ç½®ä¿¡åº¦
                - behavior_type: è¡Œä¸ºç±»å‹ (normal/warning)
                - alert_level: é¢„è­¦çº§åˆ« (0-3)
            alert_triggered: æ˜¯å¦è§¦å‘é¢„è­¦
            
        Returns:
            è®°å½•ID
        """
        if self._current_session_id is None:
            raise RuntimeError("No active session. Call start_session() first.")
        
        self._frame_count += 1
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        record = {
            'session_id': self._current_session_id,
            'frame_id': frame_id,
            'timestamp': timestamp,
            'alert_triggered': alert_triggered,
            'detection_count': len(detections)
        }
        self._record_buffer.append(record)
        
        # æš‚å­˜æ£€æµ‹æ¡ç›®ï¼ˆéœ€è¦record_idï¼Œç¨åå¤„ç†ï¼‰
        for det in detections:
            entry = {
                'frame_id': frame_id,  # ä¸´æ—¶æ ‡è®°
                'bbox': det['bbox'],
                'class_id': det['class_id'],
                'class_name': det['class_name'],
                'confidence': det['confidence'],
                'behavior_type': det['behavior_type'],
                'alert_level': det.get('alert_level', 0)
            }
            self._entry_buffer.append(entry)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
        if len(self._record_buffer) >= self._buffer_size:
            self._flush_buffers()
        
        return frame_id
    
    def save_detection_batch(
        self,
        results: List[Dict[str, Any]]
    ) -> int:
        """
        æ‰¹é‡ä¿å­˜æ£€æµ‹ç»“æœ
        
        Args:
            results: æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«:
                - frame_id: å¸§ID
                - timestamp: æ—¶é—´æˆ³
                - detections: æ£€æµ‹åˆ—è¡¨
                - alert_triggered: æ˜¯å¦è§¦å‘é¢„è­¦
                
        Returns:
            ä¿å­˜çš„è®°å½•æ•°
        """
        for result in results:
            self.save_detection_result(
                frame_id=result['frame_id'],
                timestamp=result['timestamp'],
                detections=result.get('detections', []),
                alert_triggered=result.get('alert_triggered', False)
            )
        
        return len(results)
    
    def _flush_buffers(self) -> None:
        """åˆ·æ–°ç¼“å†²åŒºåˆ°æ•°æ®åº“"""
        if not self._record_buffer:
            return
        
        # ä½¿ç”¨repositoryæ¨¡å¼ï¼Œè€Œä¸æ˜¯ç›´æ¥æ•°æ®åº“æ“ä½œ
        try:
            # æ‰¹é‡ä¿å­˜è®°å½•
            record_ids = []
            for record in self._record_buffer:
                record_id = self.data_access.detection_repo.create_record(
                    session_id=record['session_id'],
                    frame_id=record['frame_id'],
                    timestamp=record['timestamp'],
                    alert_triggered=record['alert_triggered'],
                    detection_count=record['detection_count']
                )
                record_ids.append(record_id)
            
            # æ‰¹é‡ä¿å­˜æ¡ç›®
            if self._entry_buffer and record_ids:
                # æ„å»ºframe_idåˆ°record_idçš„æ˜ å°„
                frame_to_record = {}
                for i, record in enumerate(self._record_buffer):
                    if i < len(record_ids):
                        frame_to_record[record['frame_id']] = record_ids[i]
                
                # ä¿å­˜æ¡ç›®
                for entry in self._entry_buffer:
                    record_id = frame_to_record.get(entry['frame_id'])
                    if record_id:
                        self.data_access.detection_repo.create_entry(
                            record_id=record_id,
                            bbox=entry['bbox'],
                            class_id=entry['class_id'],
                            class_name=entry['class_name'],
                            confidence=entry['confidence'],
                            behavior_type=entry['behavior_type'],
                            alert_level=entry['alert_level']
                        )
        
        except Exception as e:
            logger.error(f"Failed to flush buffers: {e}")
            raise
        finally:
            # æ¸…ç©ºç¼“å†²åŒº
            self._record_buffer = []
            self._entry_buffer = []
    
    def save_alert_result(self, alert_result: Any, frame_id: int = None) -> int:
        """
        ä¿å­˜AlertResultå¯¹è±¡
        
        Args:
            alert_result: AlertResultå¯¹è±¡ï¼ˆæ¥è‡ªalertæ¨¡å—ï¼‰
            frame_id: å¸§IDï¼ˆå¦‚æœAlertResultä¸­æ²¡æœ‰ï¼‰
            
        Returns:
            è®°å½•ID
        """
        # ä»AlertResultæå–æ•°æ®
        detections = []
        for det in alert_result.detections:
            detections.append({
                'bbox': det.bbox,
                'class_id': det.class_id,
                'class_name': det.class_name,
                'confidence': det.confidence,
                'behavior_type': det.behavior_type,
                'alert_level': det.alert_level
            })
        
        return self.save_detection_result(
            frame_id=frame_id or alert_result.frame_id,
            timestamp=alert_result.timestamp,
            detections=detections,
            alert_triggered=alert_result.alert_triggered
        )
    
    def get_session_statistics(self, session_id: int) -> Dict[str, Any]:
        """è·å–ä¼šè¯ç»Ÿè®¡"""
        return self.data_access.get_session_statistics(session_id)
    
    def get_session_detections(
        self,
        session_id: int,
        behavior_type: str = None,
        alert_level: int = None
    ) -> List[Dict[str, Any]]:
        """è·å–ä¼šè¯çš„æ£€æµ‹ç»“æœ"""
        return self.data_access.get_behavior_entries(
            session_id=session_id,
            behavior_type=behavior_type,
            alert_level=alert_level
        )
    
    def export_session_json(self, session_id: int) -> str:
        """å¯¼å‡ºä¼šè¯æ•°æ®ä¸ºJSON"""
        return self.data_access.export_session_to_json(session_id)
    
    def close(self) -> None:
        """å…³é—­æœåŠ¡"""
        if self._current_session_id:
            self.end_session(status='failed')
        self.data_access.close()
    
    def __enter__(self) -> 'DetectionService':
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        self.close()


# å…¨å±€æ£€æµ‹æœåŠ¡å®ä¾‹
_detection_service: Optional[DetectionService] = None

def get_detection_service() -> DetectionService:
    """è·å–æ£€æµ‹æœåŠ¡å•ä¾‹"""
    global _detection_service
    if _detection_service is None:
        _detection_service = DetectionService()
    return _detection_service
