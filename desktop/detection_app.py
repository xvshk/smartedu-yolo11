"""
PySide6 å®æ—¶è¯¾å ‚è¡Œä¸ºæ£€æµ‹åº”ç”¨
æ”¯æŒæ‘„åƒå¤´å’Œè§†é¢‘æ–‡ä»¶æ£€æµ‹ï¼Œä½¿ç”¨ YOLO æ¨¡å‹è¿›è¡Œè¡Œä¸ºè¯†åˆ«
æ£€æµ‹ç»“æœè‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“
"""
import sys
import os
import cv2
import json
import numpy as np
from datetime import datetime
from typing import Optional, Dict, List, Any
from dataclasses import dataclass, asdict
import requests

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from PySide6.QtWidgets import (
    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
    QPushButton, QLabel, QComboBox, QSlider, QGroupBox, QFileDialog,
    QTableWidget, QTableWidgetItem, QHeaderView, QSplitter, QFrame,
    QProgressBar, QMessageBox, QSpinBox, QCheckBox, QLineEdit
)
from PySide6.QtCore import Qt, QTimer, Signal, QThread, Slot
from PySide6.QtGui import QImage, QPixmap, QFont, QColor, QPalette

# è¡Œä¸ºç±»åˆ«é…ç½®
BEHAVIOR_CLASSES = {
    0: {'name': 'handrise', 'cn_name': 'ä¸¾æ‰‹', 'type': 'normal', 'color': (0, 255, 0)},
    2: {'name': 'write', 'cn_name': 'ä¹¦å†™', 'type': 'normal', 'color': (0, 180, 0)},
    3: {'name': 'sleep', 'cn_name': 'ç¡è§‰', 'type': 'warning', 'color': (255, 0, 0)},
    4: {'name': 'stand', 'cn_name': 'ç«™ç«‹', 'type': 'warning', 'color': (128, 128, 128)},
    5: {'name': 'using_electronic_devices', 'cn_name': 'ä½¿ç”¨ç”µå­è®¾å¤‡', 'type': 'warning', 'color': (255, 0, 255)},
    6: {'name': 'talk', 'cn_name': 'äº¤è°ˆ', 'type': 'warning', 'color': (255, 165, 0)},
    7: {'name': 'head_down', 'cn_name': 'ä½å¤´', 'type': 'warning', 'color': (255, 128, 0)},
}

# ç”µå­è®¾å¤‡ç±»åˆ«ï¼ˆCOCOï¼‰
ELECTRONIC_DEVICE_CLASSES = {
    67: 'cell phone',
    63: 'laptop',
}

# åç«¯ API åœ°å€
API_BASE_URL = "http://127.0.0.1:5000/api"


@dataclass
class Detection:
    """æ£€æµ‹ç»“æœ"""
    class_id: int
    class_name: str
    class_name_cn: str
    confidence: float
    bbox: List[float]
    behavior_type: str
    
    def to_dict(self) -> Dict:
        return asdict(self)


class DetectionThread(QThread):
    """æ£€æµ‹çº¿ç¨‹"""
    frame_ready = Signal(np.ndarray, list)
    fps_updated = Signal(float)
    error_occurred = Signal(str)
    session_created = Signal(int)  # ä¼šè¯ID
    
    # COCO äººä½“ç±»åˆ«ID
    PERSON_CLASS_ID = 0
    
    def __init__(self):
        super().__init__()
        self.running = False
        self.cap = None
        self.model = None
        self.device_model = None
        self.face_cascade = None  # äººè„¸æ£€æµ‹å™¨
        self.profile_cascade = None  # ä¾§è„¸æ£€æµ‹å™¨
        self.confidence_threshold = 0.35
        self.source = 0
        self.device = 'cpu'
        self.session_id = None
        self.save_to_db = True
        self.save_interval = 30  # æ¯30å¸§ä¿å­˜ä¸€æ¬¡
        self.frame_count = 0
        
        # ä½å¤´æ£€æµ‹ç›¸å…³å‚æ•°
        self.head_down_history = {}  # è®°å½•æ¯ä¸ªäººçš„ä½å¤´å†å² {person_id: [is_head_down, ...]}
        self.head_down_confirm_frames = 3  # è¿ç»­Nå¸§ç¡®è®¤æ‰åˆ¤å®šä¸ºä½å¤´
        self.head_down_min_confidence = 0.55  # ä½å¤´æ£€æµ‹æœ€ä½ç½®ä¿¡åº¦
        
        self._load_models()
        self._load_face_detector()
    
    def _load_models(self):
        """åŠ è½½ YOLO æ¨¡å‹"""
        try:
            from ultralytics import YOLO
            import torch
            
            if torch.cuda.is_available():
                self.device = 'cuda:0'
                print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
            else:
                self.device = 'cpu'
                print("ä½¿ç”¨ CPU")
            
            model_path = os.path.join(project_root, 'runs/detect/classroom_behavior_4050/weights/best.pt')
            if os.path.exists(model_path):
                self.model = YOLO(model_path)
                self.model.to(self.device)
                print(f"å·²åŠ è½½è¡Œä¸ºæ£€æµ‹æ¨¡å‹: {model_path}")
            
            device_model_paths = [
                os.path.join(project_root, 'yolo11n.pt'),
                os.path.join(project_root, 'yolo11s.pt'),
            ]
            for path in device_model_paths:
                if os.path.exists(path):
                    self.device_model = YOLO(path)
                    self.device_model.to(self.device)
                    print(f"å·²åŠ è½½ç”µå­è®¾å¤‡æ£€æµ‹æ¨¡å‹: {path}")
                    break
                    
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            self.error_occurred.emit(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    def _load_face_detector(self):
        """åŠ è½½äººè„¸æ£€æµ‹å™¨ï¼ˆç”¨äºä½å¤´æ£€æµ‹ï¼‰"""
        try:
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            
            profile_path = cv2.data.haarcascades + 'haarcascade_profileface.xml'
            self.profile_cascade = cv2.CascadeClassifier(profile_path)
            
            print("äººè„¸æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"åŠ è½½äººè„¸æ£€æµ‹å™¨å¤±è´¥: {e}")
            self.face_cascade = None
            self.profile_cascade = None
    
    def set_source(self, source):
        self.source = source
    
    def set_confidence(self, conf: float):
        self.confidence_threshold = conf
    
    def set_save_to_db(self, save: bool):
        self.save_to_db = save
    
    def create_session(self, class_id: int = None) -> Optional[int]:
        """åˆ›å»ºæ£€æµ‹ä¼šè¯"""
        try:
            response = requests.post(f"{API_BASE_URL}/detection/session/start", json={
                "class_id": class_id,
                "source_type": "pyside6_realtime"
            }, timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get('success'):
                    self.session_id = data['data']['session_id']
                    print(f"åˆ›å»ºæ£€æµ‹ä¼šè¯: {self.session_id}")
                    return self.session_id
        except Exception as e:
            print(f"åˆ›å»ºä¼šè¯å¤±è´¥: {e}")
        return None
    
    def end_session(self):
        """ç»“æŸæ£€æµ‹ä¼šè¯"""
        if self.session_id:
            try:
                requests.post(f"{API_BASE_URL}/detection/session/end", json={
                    "session_id": self.session_id
                }, timeout=5)
                print(f"ç»“æŸæ£€æµ‹ä¼šè¯: {self.session_id}")
            except Exception as e:
                print(f"ç»“æŸä¼šè¯å¤±è´¥: {e}")
            self.session_id = None
    
    def save_detection_result(self, detections: List[Detection]):
        """ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“"""
        if not self.save_to_db or not self.session_id:
            return
        
        try:
            detection_data = {
                "session_id": self.session_id,
                "detections": [d.to_dict() for d in detections],
                "total_count": len(detections),
                "warning_count": sum(1 for d in detections if d.behavior_type == 'warning'),
                "normal_count": sum(1 for d in detections if d.behavior_type == 'normal'),
                "behavior_summary": {},
                "timestamp": datetime.now().isoformat()
            }
            
            # ç»Ÿè®¡è¡Œä¸º
            for d in detections:
                name = d.class_name_cn
                if 'ä½¿ç”¨ç”µå­è®¾å¤‡' in name:
                    name = 'ä½¿ç”¨ç”µå­è®¾å¤‡'
                detection_data["behavior_summary"][name] = detection_data["behavior_summary"].get(name, 0) + 1
            
            requests.post(f"{API_BASE_URL}/detection/save", json=detection_data, timeout=3)
        except Exception as e:
            print(f"ä¿å­˜æ£€æµ‹ç»“æœå¤±è´¥: {e}")
    
    def run(self):
        self.running = True
        self.frame_count = 0
        
        # åˆ›å»ºä¼šè¯
        if self.save_to_db:
            session_id = self.create_session()
            if session_id:
                self.session_created.emit(session_id)
        
        # æ‰“å¼€è§†é¢‘æº
        if isinstance(self.source, str) and os.path.exists(self.source):
            self.cap = cv2.VideoCapture(self.source)
        else:
            self.cap = cv2.VideoCapture(int(self.source) if str(self.source).isdigit() else 0)
        
        if not self.cap.isOpened():
            self.error_occurred.emit("æ— æ³•æ‰“å¼€è§†é¢‘æº")
            return
        
        frame_count = 0
        start_time = datetime.now()
        
        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                if isinstance(self.source, str):
                    self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    continue
                break
            
            detections = self._detect(frame)
            annotated_frame = self._draw_detections(frame, detections)
            self.frame_ready.emit(annotated_frame, detections)
            
            # å®šæœŸä¿å­˜åˆ°æ•°æ®åº“
            self.frame_count += 1
            if self.save_to_db and self.frame_count % self.save_interval == 0:
                self.save_detection_result(detections)
            
            frame_count += 1
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed >= 1.0:
                fps = frame_count / elapsed
                self.fps_updated.emit(fps)
                frame_count = 0
                start_time = datetime.now()
        
        # ç»“æŸä¼šè¯
        if self.save_to_db:
            self.end_session()
        
        if self.cap:
            self.cap.release()
    
    def _detect(self, frame: np.ndarray) -> List[Detection]:
        detections = []
        person_boxes = []  # äººä½“è¾¹ç•Œæ¡†ï¼ˆç”¨äºä½å¤´æ£€æµ‹ï¼‰
        
        if self.model is not None:
            try:
                results = self.model(frame, conf=self.confidence_threshold, iou=0.5, verbose=False)
                for result in results:
                    boxes = result.boxes
                    if boxes is not None:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            if cls_id in BEHAVIOR_CLASSES:
                                class_info = BEHAVIOR_CLASSES[cls_id]
                                detections.append(Detection(
                                    class_id=cls_id,
                                    class_name=class_info['name'],
                                    class_name_cn=class_info['cn_name'],
                                    confidence=conf,
                                    bbox=xyxy,
                                    behavior_type=class_info['type']
                                ))
            except Exception as e:
                print(f"è¡Œä¸ºæ£€æµ‹é”™è¯¯: {e}")
        
        if self.device_model is not None:
            try:
                results = self.device_model(frame, conf=0.3, iou=0.5, verbose=False)
                for result in results:
                    boxes = result.boxes
                    if boxes is not None:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            # æ£€æµ‹ç”µå­è®¾å¤‡ - æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æ£€æµ‹æ¡†é‡å 
                            if cls_id in ELECTRONIC_DEVICE_CLASSES:
                                # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰è¡Œä¸ºæ£€æµ‹æ¡†é‡å 
                                if not self._is_overlapping(xyxy, detections, threshold=0.3):
                                    device_name = ELECTRONIC_DEVICE_CLASSES[cls_id]
                                    detections.append(Detection(
                                        class_id=5,
                                        class_name='using_electronic_devices',
                                        class_name_cn=f'ä½¿ç”¨ç”µå­è®¾å¤‡({device_name})',
                                        confidence=conf,
                                        bbox=xyxy,
                                        behavior_type='warning'
                                    ))
                            
                            # æ£€æµ‹äººä½“ï¼ˆç”¨äºä½å¤´æ£€æµ‹ï¼‰
                            if cls_id == self.PERSON_CLASS_ID and conf > 0.4:
                                person_boxes.append(xyxy)
            except Exception as e:
                print(f"ç”µå­è®¾å¤‡æ£€æµ‹é”™è¯¯: {e}")
        
        # ä½å¤´æ£€æµ‹
        if person_boxes and self.face_cascade is not None:
            head_down_results = self._detect_head_down(frame, person_boxes, detections)
            for hd in head_down_results:
                # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æ£€æµ‹æ¡†é‡å 
                if not self._is_overlapping(hd['bbox'], detections, threshold=0.3):
                    head_down_class_info = BEHAVIOR_CLASSES[7]  # head_down
                    detections.append(Detection(
                        class_id=7,
                        class_name=head_down_class_info['name'],
                        class_name_cn=head_down_class_info['cn_name'],
                        confidence=hd['confidence'],
                        bbox=hd['bbox'],
                        behavior_type=head_down_class_info['type']
                    ))
        
        # æœ€ç»ˆå»é‡ï¼šç§»é™¤é‡å çš„æ£€æµ‹æ¡†ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
        detections = self._remove_duplicate_detections(detections)
        
        return detections
    
    def _is_overlapping(self, bbox: List[float], detections: List[Detection], threshold: float = 0.3) -> bool:
        """æ£€æŸ¥è¾¹ç•Œæ¡†æ˜¯å¦ä¸å·²æœ‰æ£€æµ‹æ¡†é‡å """
        x1, y1, x2, y2 = bbox
        box_area = (x2 - x1) * (y2 - y1)
        
        for det in detections:
            dx1, dy1, dx2, dy2 = det.bbox
            
            # è®¡ç®—äº¤é›†
            inter_x1 = max(x1, dx1)
            inter_y1 = max(y1, dy1)
            inter_x2 = min(x2, dx2)
            inter_y2 = min(y2, dy2)
            
            if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                det_area = (dx2 - dx1) * (dy2 - dy1)
                
                # è®¡ç®—IoU
                union_area = box_area + det_area - inter_area
                iou = inter_area / union_area if union_area > 0 else 0
                
                if iou > threshold:
                    return True
        
        return False
    
    def _remove_duplicate_detections(self, detections: List[Detection]) -> List[Detection]:
        """ç§»é™¤é‡å çš„æ£€æµ‹æ¡†ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„"""
        if len(detections) <= 1:
            return detections
        
        # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
        sorted_dets = sorted(detections, key=lambda x: x.confidence, reverse=True)
        keep = []
        
        for det in sorted_dets:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²ä¿ç•™çš„æ£€æµ‹æ¡†é‡å 
            is_duplicate = False
            for kept in keep:
                x1, y1, x2, y2 = det.bbox
                kx1, ky1, kx2, ky2 = kept.bbox
                
                # è®¡ç®—äº¤é›†
                inter_x1 = max(x1, kx1)
                inter_y1 = max(y1, ky1)
                inter_x2 = min(x2, kx2)
                inter_y2 = min(y2, ky2)
                
                if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                    det_area = (x2 - x1) * (y2 - y1)
                    kept_area = (kx2 - kx1) * (ky2 - ky1)
                    
                    # è®¡ç®—IoU
                    union_area = det_area + kept_area - inter_area
                    iou = inter_area / union_area if union_area > 0 else 0
                    
                    # å¦‚æœIoU > 0.4ï¼Œè®¤ä¸ºæ˜¯åŒä¸€ä¸ªäººçš„é‡å¤æ£€æµ‹
                    if iou > 0.4:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                keep.append(det)
        
        return keep
    
    def _detect_head_down(self, image: np.ndarray, person_boxes: List[List[float]], 
                          existing_detections: List[Detection]) -> List[Dict]:
        """
        æ”¹è¿›çš„ä½å¤´æ£€æµ‹ç®—æ³•
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. åªæ£€æµ‹è¿‘è·ç¦»çš„å¤§ç›®æ ‡ï¼ˆé¿å…è¿œå¤„å°ç›®æ ‡è¯¯æ£€ï¼‰
        2. æ£€æµ‹æ•´ä¸ªäººä½“åŒºåŸŸçš„äººè„¸ï¼Œè€Œä¸ä»…ä»…æ˜¯å¤´éƒ¨åŒºåŸŸ
        3. å¦‚æœåœ¨æ•´ä¸ªäººä½“åŒºåŸŸéƒ½æ£€æµ‹ä¸åˆ°äººè„¸ï¼Œæ‰åˆ¤å®šä¸ºä½å¤´
        4. å¢åŠ æ›´å¤šè¿‡æ»¤æ¡ä»¶å‡å°‘è¯¯æ£€
        
        Args:
            image: å›¾åƒ
            person_boxes: äººä½“è¾¹ç•Œæ¡†åˆ—è¡¨
            existing_detections: å·²æœ‰çš„æ£€æµ‹ç»“æœ
            
        Returns:
            ä½å¤´æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        head_down_detections = []
        
        if self.face_cascade is None:
            return head_down_detections
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        h, w = image.shape[:2]
        
        # è·å–å·²æ£€æµ‹åˆ°çš„è¡Œä¸ºåŒºåŸŸï¼ˆæ’é™¤ä½å¤´ï¼‰
        existing_boxes = []
        if existing_detections:
            for det in existing_detections:
                # æ’é™¤å·²æ£€æµ‹åˆ°çš„è¡Œä¸º
                if det.class_id in [0, 2, 3, 4, 5, 6]:
                    existing_boxes.append(det.bbox)
        
        for person_box in person_boxes:
            x1, y1, x2, y2 = [int(v) for v in person_box]
            
            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            x1 = max(0, x1)
            y1 = max(0, y1)
            x2 = min(w, x2)
            y2 = min(h, y2)
            
            if x2 <= x1 or y2 <= y1:
                continue
            
            person_height = y2 - y1
            person_width = x2 - x1
            
            # ä¸¥æ ¼è¿‡æ»¤æ¡ä»¶1ï¼šåªæ£€æµ‹è¶³å¤Ÿå¤§çš„ç›®æ ‡ï¼ˆè¿‘è·ç¦»ï¼‰
            # äººä½“æ¡†å¿…é¡»å å›¾åƒé«˜åº¦çš„30%ä»¥ä¸Š
            if person_height < h * 0.3:
                continue
            
            # ä¸¥æ ¼è¿‡æ»¤æ¡ä»¶2ï¼šäººä½“æ¡†å¿…é¡»è¶³å¤Ÿå¤§ï¼ˆç»å¯¹å°ºå¯¸ï¼‰
            if person_height < 200 or person_width < 100:
                continue
            
            # ä¸¥æ ¼è¿‡æ»¤æ¡ä»¶3ï¼šå®½é«˜æ¯”æ£€æŸ¥
            aspect_ratio = person_width / person_height
            if aspect_ratio > 1.2 or aspect_ratio < 0.25:
                continue
            
            # ä¸¥æ ¼è¿‡æ»¤æ¡ä»¶4ï¼šæ£€æŸ¥æ˜¯å¦ä¸å·²æ£€æµ‹è¡Œä¸ºåŒºåŸŸé‡å 
            skip_person = False
            for eb in existing_boxes:
                ex1, ey1, ex2, ey2 = [int(v) for v in eb]
                inter_x1 = max(x1, ex1)
                inter_y1 = max(y1, ey1)
                inter_x2 = min(x2, ex2)
                inter_y2 = min(y2, ey2)
                
                if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                    person_area = (x2 - x1) * (y2 - y1)
                    if inter_area / person_area > 0.15:
                        skip_person = True
                        break
            
            if skip_person:
                continue
            
            # åœ¨æ•´ä¸ªäººä½“ä¸ŠåŠéƒ¨åˆ†åŒºåŸŸæ£€æµ‹äººè„¸ï¼ˆæ‰©å¤§æ£€æµ‹èŒƒå›´åˆ°50%ï¼‰
            head_y2 = y1 + int(person_height * 0.5)
            person_region = gray[y1:head_y2, x1:x2]
            
            if person_region.size == 0:
                continue
            
            # ä½¿ç”¨æ›´å®½æ¾çš„å‚æ•°æ£€æµ‹äººè„¸ï¼ˆå‡å°‘æ¼æ£€ï¼‰
            faces = self.face_cascade.detectMultiScale(
                person_region,
                scaleFactor=1.1,
                minNeighbors=3,  # é™ä½ä»¥æé«˜æ£€æµ‹ç‡
                minSize=(20, 20),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            # å¦‚æœæ£€æµ‹åˆ°äººè„¸ï¼Œè¯´æ˜ä¸æ˜¯ä½å¤´
            if len(faces) > 0:
                continue
            
            # å°è¯•æ£€æµ‹ä¾§è„¸
            if self.profile_cascade is not None:
                profiles = self.profile_cascade.detectMultiScale(
                    person_region,
                    scaleFactor=1.1,
                    minNeighbors=3,
                    minSize=(20, 20),
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                
                if len(profiles) > 0:
                    continue
                
                # ç¿»è½¬æ£€æµ‹å¦ä¸€ä¾§
                flipped = cv2.flip(person_region, 1)
                profiles_flip = self.profile_cascade.detectMultiScale(
                    flipped,
                    scaleFactor=1.1,
                    minNeighbors=3,
                    minSize=(20, 20),
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                
                if len(profiles_flip) > 0:
                    continue
            
            # æ‰€æœ‰äººè„¸æ£€æµ‹éƒ½å¤±è´¥ï¼Œåˆ¤å®šä¸ºä½å¤´
            # ç½®ä¿¡åº¦åŸºäºäººä½“æ¡†å¤§å°
            confidence = 0.6 + (person_height / h) * 0.2
            confidence = min(0.85, confidence)
            
            head_down_detections.append({
                'bbox': [x1, y1, x2, y1 + int(person_height * 0.45)],
                'confidence': round(confidence, 3),
                'reason': 'no_face_detected'
            })
        
        return head_down_detections
    
    def _draw_detections(self, frame: np.ndarray, detections: List[Detection]) -> np.ndarray:
        for det in detections:
            x1, y1, x2, y2 = [int(v) for v in det.bbox]
            
            if det.class_id in BEHAVIOR_CLASSES:
                color = BEHAVIOR_CLASSES[det.class_id]['color']
            else:
                color = (255, 0, 255)
            
            color_bgr = (color[2], color[1], color[0])
            thickness = 3 if det.behavior_type == 'warning' else 2
            cv2.rectangle(frame, (x1, y1), (x2, y2), color_bgr, thickness)
            
            label = f"{det.class_name_cn} {det.confidence:.2f}"
            (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            cv2.rectangle(frame, (x1, y1 - label_h - 10), (x1 + label_w + 10, y1), color_bgr, -1)
            
            try:
                from PIL import Image, ImageDraw, ImageFont
                pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                draw = ImageDraw.Draw(pil_img)
                
                font = None
                font_paths = [
                    "C:/Windows/Fonts/msyh.ttc",
                    "C:/Windows/Fonts/simhei.ttf",
                    "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",
                ]
                for fp in font_paths:
                    if os.path.exists(fp):
                        font = ImageFont.truetype(fp, 16)
                        break
                
                if font:
                    draw.text((x1 + 5, y1 - label_h - 8), label, fill=(255, 255, 255), font=font)
                    frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
                else:
                    cv2.putText(frame, label, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            except:
                cv2.putText(frame, label, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return frame
    
    def stop(self):
        self.running = False
        self.wait()


class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("è¯¾å ‚è¡Œä¸ºæ™ºèƒ½æ£€æµ‹ç³»ç»Ÿ - PySide6")
        self.setMinimumSize(1200, 800)
        
        self.detection_thread = DetectionThread()
        self.detection_thread.frame_ready.connect(self.update_frame)
        self.detection_thread.fps_updated.connect(self.update_fps)
        self.detection_thread.error_occurred.connect(self.show_error)
        self.detection_thread.session_created.connect(self.on_session_created)
        
        self.behavior_stats = {info['cn_name']: 0 for info in BEHAVIOR_CLASSES.values()}
        self.current_session_id = None
        
        self._setup_ui()
        self._apply_style()
    
    def _setup_ui(self):
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        main_layout = QHBoxLayout(central_widget)
        main_layout.setSpacing(10)
        main_layout.setContentsMargins(10, 10, 10, 10)
        
        # å·¦ä¾§ï¼šè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        left_panel = QWidget()
        left_layout = QVBoxLayout(left_panel)
        left_layout.setSpacing(10)
        
        self.video_label = QLabel()
        self.video_label.setMinimumSize(800, 600)
        self.video_label.setAlignment(Qt.AlignCenter)
        self.video_label.setStyleSheet("""
            QLabel {
                background-color: #1a1a2e;
                border: 2px solid #667eea;
                border-radius: 10px;
            }
        """)
        self.video_label.setText("ç‚¹å‡»ã€Œå¼€å§‹æ£€æµ‹ã€å¯åŠ¨æ‘„åƒå¤´\næˆ–é€‰æ‹©è§†é¢‘æ–‡ä»¶")
        left_layout.addWidget(self.video_label)
        
        # æ§åˆ¶æŒ‰é’®
        control_layout = QHBoxLayout()
        
        self.start_btn = QPushButton("â–¶ å¼€å§‹æ£€æµ‹")
        self.start_btn.clicked.connect(self.start_detection)
        control_layout.addWidget(self.start_btn)
        
        self.stop_btn = QPushButton("â¹ åœæ­¢æ£€æµ‹")
        self.stop_btn.clicked.connect(self.stop_detection)
        self.stop_btn.setEnabled(False)
        control_layout.addWidget(self.stop_btn)
        
        self.file_btn = QPushButton("ğŸ¬ é€‰æ‹©è§†é¢‘")
        self.file_btn.clicked.connect(self.select_video)
        control_layout.addWidget(self.file_btn)
        
        self.image_btn = QPushButton("ğŸ–¼ï¸ æ£€æµ‹å›¾ç‰‡")
        self.image_btn.clicked.connect(self.detect_image)
        control_layout.addWidget(self.image_btn)
        
        self.screenshot_btn = QPushButton("ğŸ“· æˆªå›¾")
        self.screenshot_btn.clicked.connect(self.take_screenshot)
        control_layout.addWidget(self.screenshot_btn)
        
        left_layout.addLayout(control_layout)
        
        # çŠ¶æ€æ 
        status_layout = QHBoxLayout()
        self.fps_label = QLabel("FPS: 0.0")
        self.fps_label.setStyleSheet("color: #67C23A; font-weight: bold;")
        status_layout.addWidget(self.fps_label)
        
        self.status_label = QLabel("çŠ¶æ€: å°±ç»ª")
        status_layout.addWidget(self.status_label)
        
        self.session_label = QLabel("ä¼šè¯: æœªåˆ›å»º")
        self.session_label.setStyleSheet("color: #909399;")
        status_layout.addWidget(self.session_label)
        
        status_layout.addStretch()
        left_layout.addLayout(status_layout)
        
        main_layout.addWidget(left_panel, stretch=3)
        
        # å³ä¾§ï¼šæ§åˆ¶é¢æ¿å’Œç»Ÿè®¡
        right_panel = QWidget()
        right_panel.setMaximumWidth(350)
        right_layout = QVBoxLayout(right_panel)
        right_layout.setSpacing(10)
        
        # è®¾ç½®ç»„
        settings_group = QGroupBox("æ£€æµ‹è®¾ç½®")
        settings_layout = QVBoxLayout(settings_group)
        
        # æ‘„åƒå¤´é€‰æ‹©
        cam_layout = QHBoxLayout()
        cam_layout.addWidget(QLabel("æ‘„åƒå¤´:"))
        self.camera_combo = QComboBox()
        self.camera_combo.addItems(["æ‘„åƒå¤´ 0", "æ‘„åƒå¤´ 1", "æ‘„åƒå¤´ 2"])
        cam_layout.addWidget(self.camera_combo)
        settings_layout.addLayout(cam_layout)
        
        # ç½®ä¿¡åº¦é˜ˆå€¼
        conf_layout = QHBoxLayout()
        conf_layout.addWidget(QLabel("ç½®ä¿¡åº¦:"))
        self.conf_slider = QSlider(Qt.Horizontal)
        self.conf_slider.setRange(10, 90)
        self.conf_slider.setValue(35)
        self.conf_slider.valueChanged.connect(self.update_confidence)
        conf_layout.addWidget(self.conf_slider)
        self.conf_label = QLabel("0.35")
        conf_layout.addWidget(self.conf_label)
        settings_layout.addLayout(conf_layout)
        
        # ä¿å­˜åˆ°æ•°æ®åº“é€‰é¡¹
        self.save_db_checkbox = QCheckBox("ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“")
        self.save_db_checkbox.setChecked(True)
        settings_layout.addWidget(self.save_db_checkbox)
        
        right_layout.addWidget(settings_group)
        
        # å®æ—¶ç»Ÿè®¡ç»„
        stats_group = QGroupBox("å®æ—¶ç»Ÿè®¡")
        stats_layout = QVBoxLayout(stats_group)
        
        self.stats_table = QTableWidget()
        self.stats_table.setColumnCount(2)
        self.stats_table.setHorizontalHeaderLabels(["è¡Œä¸ºç±»å‹", "æ£€æµ‹æ¬¡æ•°"])
        self.stats_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        self.stats_table.setRowCount(len(BEHAVIOR_CLASSES))
        
        for i, (cls_id, info) in enumerate(BEHAVIOR_CLASSES.items()):
            self.stats_table.setItem(i, 0, QTableWidgetItem(info['cn_name']))
            self.stats_table.setItem(i, 1, QTableWidgetItem("0"))
        
        stats_layout.addWidget(self.stats_table)
        
        self.reset_stats_btn = QPushButton("é‡ç½®ç»Ÿè®¡")
        self.reset_stats_btn.clicked.connect(self.reset_stats)
        stats_layout.addWidget(self.reset_stats_btn)
        
        right_layout.addWidget(stats_group)
        
        # å½“å‰æ£€æµ‹ç»“æœ
        current_group = QGroupBox("å½“å‰å¸§æ£€æµ‹")
        current_layout = QVBoxLayout(current_group)
        
        self.current_table = QTableWidget()
        self.current_table.setColumnCount(3)
        self.current_table.setHorizontalHeaderLabels(["è¡Œä¸º", "ç½®ä¿¡åº¦", "ç±»å‹"])
        self.current_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        current_layout.addWidget(self.current_table)
        
        right_layout.addWidget(current_group)
        right_layout.addStretch()
        
        main_layout.addWidget(right_panel, stretch=1)
    
    def _apply_style(self):
        self.setStyleSheet("""
            QMainWindow { background-color: #f5f7fa; }
            QGroupBox {
                font-weight: bold;
                border: 2px solid #667eea;
                border-radius: 8px;
                margin-top: 10px;
                padding-top: 10px;
                background-color: white;
            }
            QGroupBox::title {
                subcontrol-origin: margin;
                left: 10px;
                padding: 0 5px;
                color: #667eea;
            }
            QPushButton {
                background: qlineargradient(x1:0, y1:0, x2:1, y2:0, stop:0 #667eea, stop:1 #764ba2);
                color: white;
                border: none;
                padding: 10px 20px;
                border-radius: 6px;
                font-weight: bold;
            }
            QPushButton:hover {
                background: qlineargradient(x1:0, y1:0, x2:1, y2:0, stop:0 #764ba2, stop:1 #667eea);
            }
            QPushButton:disabled { background: #cccccc; }
            QTableWidget {
                border: 1px solid #e0e0e0;
                border-radius: 4px;
                gridline-color: #f0f0f0;
            }
            QTableWidget::item { padding: 5px; }
            QHeaderView::section {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1, stop:0 #667eea, stop:1 #764ba2);
                color: white;
                padding: 8px;
                border: none;
                font-weight: bold;
            }
            QSlider::groove:horizontal {
                height: 8px;
                background: #e0e0e0;
                border-radius: 4px;
            }
            QSlider::handle:horizontal {
                background: #667eea;
                width: 18px;
                margin: -5px 0;
                border-radius: 9px;
            }
            QComboBox {
                padding: 5px 10px;
                border: 1px solid #667eea;
                border-radius: 4px;
            }
        """)
    
    @Slot(np.ndarray, list)
    def update_frame(self, frame: np.ndarray, detections: List[Detection]):
        # è½¬æ¢BGRåˆ°RGBå¹¶ç¡®ä¿å†…å­˜è¿ç»­
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        rgb_frame = np.ascontiguousarray(rgb_frame)
        
        h, w, ch = rgb_frame.shape
        bytes_per_line = ch * w
        
        # ä½¿ç”¨RGBæ ¼å¼åˆ›å»ºQImage
        qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)
        
        # å¤åˆ¶å›¾åƒæ•°æ®é¿å…å†…å­˜é—®é¢˜
        pixmap = QPixmap.fromImage(qt_image.copy())
        
        scaled_pixmap = pixmap.scaled(
            self.video_label.size(),
            Qt.KeepAspectRatio,
            Qt.SmoothTransformation
        )
        self.video_label.setPixmap(scaled_pixmap)
        
        for det in detections:
            if det.class_name_cn in self.behavior_stats:
                self.behavior_stats[det.class_name_cn] += 1
            elif 'ä½¿ç”¨ç”µå­è®¾å¤‡' in det.class_name_cn:
                self.behavior_stats['ä½¿ç”¨ç”µå­è®¾å¤‡'] += 1
        
        for i, (cls_id, info) in enumerate(BEHAVIOR_CLASSES.items()):
            count = self.behavior_stats.get(info['cn_name'], 0)
            self.stats_table.setItem(i, 1, QTableWidgetItem(str(count)))
        
        self.current_table.setRowCount(len(detections))
        for i, det in enumerate(detections):
            self.current_table.setItem(i, 0, QTableWidgetItem(det.class_name_cn))
            self.current_table.setItem(i, 1, QTableWidgetItem(f"{det.confidence:.2f}"))
            
            type_item = QTableWidgetItem("âš ï¸ é¢„è­¦" if det.behavior_type == 'warning' else "âœ… æ­£å¸¸")
            if det.behavior_type == 'warning':
                type_item.setForeground(QColor("#F56C6C"))
            else:
                type_item.setForeground(QColor("#67C23A"))
            self.current_table.setItem(i, 2, type_item)
    
    @Slot(float)
    def update_fps(self, fps: float):
        self.fps_label.setText(f"FPS: {fps:.1f}")
    
    @Slot(str)
    def show_error(self, error: str):
        QMessageBox.critical(self, "é”™è¯¯", error)
    
    @Slot(int)
    def on_session_created(self, session_id: int):
        self.current_session_id = session_id
        self.session_label.setText(f"ä¼šè¯: #{session_id}")
        self.session_label.setStyleSheet("color: #67C23A; font-weight: bold;")
    
    def update_confidence(self, value: int):
        conf = value / 100.0
        self.conf_label.setText(f"{conf:.2f}")
        self.detection_thread.set_confidence(conf)
    
    def start_detection(self):
        source = self.camera_combo.currentIndex()
        self.detection_thread.set_source(source)
        self.detection_thread.set_save_to_db(self.save_db_checkbox.isChecked())
        self.detection_thread.start()
        
        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self.status_label.setText("çŠ¶æ€: æ£€æµ‹ä¸­...")
    
    def stop_detection(self):
        self.detection_thread.stop()
        
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        self.status_label.setText("çŠ¶æ€: å·²åœæ­¢")
        self.session_label.setText("ä¼šè¯: å·²ç»“æŸ")
        self.session_label.setStyleSheet("color: #909399;")
    
    def select_video(self):
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©è§†é¢‘æ–‡ä»¶", "",
            "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi *.mkv *.mov);;æ‰€æœ‰æ–‡ä»¶ (*.*)"
        )
        if file_path:
            self.detection_thread.set_source(file_path)
            self.status_label.setText(f"å·²é€‰æ‹©: {os.path.basename(file_path)}")
    
    def detect_image(self):
        """æ£€æµ‹å•å¼ å›¾ç‰‡"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©å›¾ç‰‡æ–‡ä»¶", "",
            "å›¾ç‰‡æ–‡ä»¶ (*.jpg *.jpeg *.png *.bmp *.webp);;æ‰€æœ‰æ–‡ä»¶ (*.*)"
        )
        if not file_path:
            return
        
        # åœæ­¢è§†é¢‘æ£€æµ‹ï¼ˆå¦‚æœæ­£åœ¨è¿è¡Œï¼‰
        if self.detection_thread.running:
            self.stop_detection()
        
        self.status_label.setText(f"æ­£åœ¨æ£€æµ‹: {os.path.basename(file_path)}")
        
        try:
            # è¯»å–å›¾ç‰‡
            image = cv2.imread(file_path)
            if image is None:
                QMessageBox.warning(self, "é”™è¯¯", "æ— æ³•è¯»å–å›¾ç‰‡æ–‡ä»¶")
                return
            
            # ä½¿ç”¨æ£€æµ‹çº¿ç¨‹çš„æ–¹æ³•è¿›è¡Œæ£€æµ‹
            detections = self._detect_single_image(image)
            
            # ç»˜åˆ¶æ£€æµ‹ç»“æœ
            annotated_image = self.detection_thread._draw_detections(image.copy(), detections)
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼å¹¶åˆ›å»ºQImage
            # ä½¿ç”¨ .copy() ç¡®ä¿æ•°æ®è¿ç»­ï¼Œé¿å…å†…å­˜é—®é¢˜
            rgb_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
            rgb_image = np.ascontiguousarray(rgb_image)
            
            h, w, ch = rgb_image.shape
            bytes_per_line = ch * w
            
            # åˆ›å»ºQImageæ—¶ä½¿ç”¨RGBæ ¼å¼
            qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
            
            # ç«‹å³è½¬æ¢ä¸ºQPixmapï¼ˆè¿™ä¼šå¤åˆ¶æ•°æ®ï¼Œé¿å…å†…å­˜é—®é¢˜ï¼‰
            pixmap = QPixmap.fromImage(qt_image.copy())
            
            scaled_pixmap = pixmap.scaled(
                self.video_label.size(),
                Qt.KeepAspectRatio,
                Qt.SmoothTransformation
            )
            self.video_label.setPixmap(scaled_pixmap)
            
            # æ›´æ–°ç»Ÿè®¡
            for det in detections:
                if det.class_name_cn in self.behavior_stats:
                    self.behavior_stats[det.class_name_cn] += 1
                elif 'ä½¿ç”¨ç”µå­è®¾å¤‡' in det.class_name_cn:
                    self.behavior_stats['ä½¿ç”¨ç”µå­è®¾å¤‡'] += 1
            
            for i, (cls_id, info) in enumerate(BEHAVIOR_CLASSES.items()):
                count = self.behavior_stats.get(info['cn_name'], 0)
                self.stats_table.setItem(i, 1, QTableWidgetItem(str(count)))
            
            # æ›´æ–°å½“å‰æ£€æµ‹è¡¨
            self.current_table.setRowCount(len(detections))
            for i, det in enumerate(detections):
                self.current_table.setItem(i, 0, QTableWidgetItem(det.class_name_cn))
                self.current_table.setItem(i, 1, QTableWidgetItem(f"{det.confidence:.2f}"))
                
                type_item = QTableWidgetItem("âš ï¸ é¢„è­¦" if det.behavior_type == 'warning' else "âœ… æ­£å¸¸")
                if det.behavior_type == 'warning':
                    type_item.setForeground(QColor("#F56C6C"))
                else:
                    type_item.setForeground(QColor("#67C23A"))
                self.current_table.setItem(i, 2, type_item)
            
            self.status_label.setText(f"æ£€æµ‹å®Œæˆ: å‘ç° {len(detections)} ä¸ªè¡Œä¸º")
            self.fps_label.setText("FPS: -")
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            QMessageBox.critical(self, "æ£€æµ‹é”™è¯¯", f"æ£€æµ‹å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
            self.status_label.setText("çŠ¶æ€: æ£€æµ‹å¤±è´¥")
    
    def _detect_single_image(self, frame: np.ndarray) -> List[Detection]:
        """æ£€æµ‹å•å¼ å›¾ç‰‡ï¼ˆå¤ç”¨æ£€æµ‹çº¿ç¨‹çš„é€»è¾‘ï¼‰"""
        detections = []
        person_boxes = []
        
        # è¡Œä¸ºæ£€æµ‹
        if self.detection_thread.model is not None:
            try:
                results = self.detection_thread.model(
                    frame, 
                    conf=self.detection_thread.confidence_threshold, 
                    iou=0.5, 
                    verbose=False
                )
                for result in results:
                    boxes = result.boxes
                    if boxes is not None:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            if cls_id in BEHAVIOR_CLASSES:
                                class_info = BEHAVIOR_CLASSES[cls_id]
                                detections.append(Detection(
                                    class_id=cls_id,
                                    class_name=class_info['name'],
                                    class_name_cn=class_info['cn_name'],
                                    confidence=conf,
                                    bbox=xyxy,
                                    behavior_type=class_info['type']
                                ))
            except Exception as e:
                print(f"è¡Œä¸ºæ£€æµ‹é”™è¯¯: {e}")
        
        # ç”µå­è®¾å¤‡æ£€æµ‹
        if self.detection_thread.device_model is not None:
            try:
                results = self.detection_thread.device_model(frame, conf=0.3, iou=0.5, verbose=False)
                for result in results:
                    boxes = result.boxes
                    if boxes is not None:
                        for box in boxes:
                            cls_id = int(box.cls[0])
                            conf = float(box.conf[0])
                            xyxy = box.xyxy[0].tolist()
                            
                            if cls_id in ELECTRONIC_DEVICE_CLASSES:
                                if not self.detection_thread._is_overlapping(xyxy, detections, threshold=0.3):
                                    device_name = ELECTRONIC_DEVICE_CLASSES[cls_id]
                                    detections.append(Detection(
                                        class_id=5,
                                        class_name='using_electronic_devices',
                                        class_name_cn=f'ä½¿ç”¨ç”µå­è®¾å¤‡({device_name})',
                                        confidence=conf,
                                        bbox=xyxy,
                                        behavior_type='warning'
                                    ))
                            
                            if cls_id == self.detection_thread.PERSON_CLASS_ID and conf > 0.4:
                                person_boxes.append(xyxy)
            except Exception as e:
                print(f"ç”µå­è®¾å¤‡æ£€æµ‹é”™è¯¯: {e}")
        
        # ä½å¤´æ£€æµ‹
        if person_boxes and self.detection_thread.face_cascade is not None:
            head_down_results = self.detection_thread._detect_head_down(frame, person_boxes, detections)
            for hd in head_down_results:
                if not self.detection_thread._is_overlapping(hd['bbox'], detections, threshold=0.3):
                    head_down_class_info = BEHAVIOR_CLASSES[7]
                    detections.append(Detection(
                        class_id=7,
                        class_name=head_down_class_info['name'],
                        class_name_cn=head_down_class_info['cn_name'],
                        confidence=hd['confidence'],
                        bbox=hd['bbox'],
                        behavior_type=head_down_class_info['type']
                    ))
        
        # å»é‡
        detections = self.detection_thread._remove_duplicate_detections(detections)
        
        return detections
    
    def take_screenshot(self):
        pixmap = self.video_label.pixmap()
        if pixmap:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"screenshot_{timestamp}.png"
            pixmap.save(filename)
            QMessageBox.information(self, "æˆªå›¾æˆåŠŸ", f"å·²ä¿å­˜: {filename}")
    
    def reset_stats(self):
        self.behavior_stats = {info['cn_name']: 0 for info in BEHAVIOR_CLASSES.values()}
        for i in range(self.stats_table.rowCount()):
            self.stats_table.setItem(i, 1, QTableWidgetItem("0"))
    
    def closeEvent(self, event):
        self.detection_thread.stop()
        event.accept()


def main():
    app = QApplication(sys.argv)
    app.setStyle('Fusion')
    
    window = MainWindow()
    window.show()
    
    sys.exit(app.exec())


if __name__ == '__main__':
    main()
